{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "jvTzv024CFxE"
      },
      "cell_type": "markdown",
      "source": [
        "# Get BBC Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-Ss8zFVjCxhN",
        "outputId": "f6a87d71-9c91-4540-fd9e-50dc0ab16204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"./gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X33wMiJNDV5U",
        "outputId": "32db1445-915b-4ad9-f272-a1e32aa187aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd gdrive/My\\ Drive/Colab\\ Notebooks/Datasets/Text\\ Summarization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Datasets/Text Summarization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eXmwymrPDflS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "with open('myfile.pkl','rb') as fp:\n",
        "  heading, article = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "szyhdhBkEVgi"
      },
      "cell_type": "markdown",
      "source": [
        "**Data from BBC News Dataset Loaded. The variables are *heading* and *article***"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6eaGtlz-E3Yx"
      },
      "cell_type": "markdown",
      "source": [
        "# Preprocess the Data , remove the \\n\\n and \\xa0 , lower case everything  (stemming/lemmatization advantageous? Try it out later)\n",
        "\n",
        "## I think I should keep the comma's in for now , maybe the Attention mechanism later would find words near a comma a good place to give attention to"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xVg5fSryGfAF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "heading = [h.lower() for h in heading]\n",
        "article = [a.lower() for a in article]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7wC_iQj2GmP0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "article = [a.replace('\\n','').replace('\\xa0',' ') for a in article]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pDC4tH40I-2C"
      },
      "cell_type": "markdown",
      "source": [
        "# Toeknize the data , Make Word Embeddings from GloVe\n",
        "\n",
        "## Mainly what I have to do is make a matrix of size (vocab_size, 100) which I can initialize as my weight matrix. Be careful, for now I am assuming no new words can appear in a text other than the ones I have seen before , so later if this works improve it to take in other words too"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "J4THBv0WJF6k",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_dimensions = 100\n",
        "import pandas as pd\n",
        "df = pd.read_csv('glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nE9viBdPNO1r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove = {key: val.values for key, val in df.T.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "g1YyKHeQFjJ0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "m3kYN7skNU2G",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = []\n",
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "for a in article:\n",
        "  tokenized_text = tokenizer.tokenize(a)\n",
        "  for tokens in tokenized_text:\n",
        "    words.append(tokens)\n",
        "    \n",
        "for h in heading:\n",
        "  tokenized_text = tokenizer.tokenize(h)\n",
        "  for tokens in tokenized_text:\n",
        "    words.append(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7QK1w4y1OAW0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wordcounts = Counter(words).most_common(170000)\n",
        "vocab_list = np.array([word for word,_ in wordcounts])\n",
        "dictionary_word_code = {word:code for code,word in enumerate(vocab_list)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uDvTMhHeBgNy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_list = []\n",
        "i = 0\n",
        "while i != vocab_list.shape[0] :\n",
        "  try:\n",
        "    embedding_list.append(glove[vocab_list[i]])\n",
        "    i = i+1\n",
        "  except KeyError as e:\n",
        "    embedding_list.append(np.random.normal(0,0.1,[word_dimensions]))\n",
        "    i = i+1\n",
        "    \n",
        "  if i%1000 == 0:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "psAAqFnZX6w_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.vstack(embedding_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8XY8MVbKYsGg"
      },
      "cell_type": "markdown",
      "source": [
        "**Embedding Matrix is created and in the variable *embedding_matrix***"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XSQyd9qvY_wT"
      },
      "cell_type": "markdown",
      "source": [
        "# Making a Bi-Directional RNN with Attention , no dropout layer\n",
        "\n",
        "## For now making a 3 Layer LSTM deep network (could cause vanishing gradient problem) and not using the EOS thing, will make the summary a fixed number of words , mostly wont be coherent but lets see if any output comes at all first."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Hh-3a2AHX8U7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PE4eDmUgA6Vz"
      },
      "cell_type": "markdown",
      "source": [
        "#### 'The' also has a code of 0 and you are gonna zero pad things so could lead to problems\n",
        "\n",
        "#### Also you are using too much RAM afterwards change variable names so you overwriting them and not keep making new ones\n",
        "Ok so for saving RAM I am thinking I will make a pickle file for all the important stuff and load it in so all the useless variables dont take up space"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SkCdPTRG1b5I",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_articles = []\n",
        "tokenized_headings = []\n",
        "for a in article:\n",
        "  tokens = tokenizer.tokenize(a)\n",
        "  tokenized_articles.append(tokens)\n",
        "for h in heading:\n",
        "  tokens = tokenizer.tokenize(h)\n",
        "  tokenized_headings.append(tokens)\n",
        "  \n",
        "max_length_article = max([len(a) for a in tokenized_articles])\n",
        "max_length_heading = max([len(h) for h in tokenized_headings])\n",
        "\n",
        "numerical_articles = []\n",
        "numerical_headings = []\n",
        "for a in tokenized_articles:\n",
        "    list1 = [dictionary_word_code[b] for b in a]\n",
        "    numerical_articles.append(list1)\n",
        "for h in tokenized_headings:\n",
        "  list1 = [dictionary_word_code[b] for b in h]\n",
        "  numerical_headings.append(list1)\n",
        "  \n",
        "padded_numerical_articles = []\n",
        "padded_numerical_headings = []\n",
        "for a in numerical_articles:\n",
        "  zeros = [0]*(max_length_article - len(a))\n",
        "  list1 = a + zeros\n",
        "  padded_numerical_articles.append(list1)\n",
        "for h in numerical_headings:\n",
        "  zeros = [0]*(max_length_heading - len(h))\n",
        "  list1 = h + zeros\n",
        "  padded_numerical_headings.append(list1)\n",
        "  \n",
        "with open(\"input_data.pickle\", \"wb\") as f:\n",
        "    pickle.dump((embedding_matrix,padded_numerical_articles,padded_numerical_headings,vocab_list), f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "A0eeO97yHjAp"
      },
      "cell_type": "markdown",
      "source": [
        "# Too much RAM was used so start from here"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1nfbmW_XAPL8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "with open(\"input_data.pickle\",\"rb\") as f:\n",
        "  _ , articles , headings, vocab_list = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "13xfVE8i54mR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def next_batch(articles,headings,batch_size,num_time_steps = 850,num_output_steps = 84):\n",
        "  \n",
        "  snipped_articles = []\n",
        "  snipped_headings = []\n",
        "  \n",
        "  for a in articles:\n",
        "    part_article = a[0:num_time_steps]\n",
        "    snipped_articles.append(part_article)\n",
        "    \n",
        "  random = np.random.randint(0,len(articles) - batch_size -1)\n",
        "  batch_x = np.array(snipped_articles[random:random+batch_size]).reshape([batch_size,num_time_steps,1])\n",
        "  onehotlistarticles = []\n",
        "  for article in batch_x:\n",
        "    for num in article:\n",
        "      zeros = np.array([0]*len(vocab_list))\n",
        "      zeros[num] = 1\n",
        "      onehotlistarticles.append(zeros)\n",
        "  onehotlistarticles = np.array(onehotlistarticles).reshape([batch_size,num_time_steps,-1])\n",
        "  \n",
        "  for h in headings:\n",
        "    part_heading = h[0:num_output_steps]\n",
        "    snipped_headings.append(part_heading)\n",
        "  \n",
        "  batch_y = np.array(snipped_headings[random:random+batch_size]).reshape([batch_size,num_output_steps,1])\n",
        "  onehotlistheadings = []\n",
        "  for heading in batch_y:\n",
        "    for num in heading:\n",
        "      zeros = np.array([0]*len(vocab_list))\n",
        "      zeros[num] = 1\n",
        "      onehotlistheadings.append(zeros)\n",
        "  onehotlistheadings = np.array(onehotlistheadings).reshape([batch_size,num_output_steps,-1])\n",
        "  \n",
        "  return onehotlistarticles , onehotlistheadings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Qhn7_KS4xxsK"
      },
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Graph \n",
        "\n",
        "## Encoder "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AMtm5eFZmJ4_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def length(sequence):\n",
        "  used = tf.reduce_max(sequence, 2)\n",
        "  length = tf.reduce_sum(used, 1)\n",
        "  length = tf.cast(length, tf.int32)\n",
        "  return length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vHspC9d0uAdB"
      },
      "cell_type": "markdown",
      "source": [
        "**See Imext Notebook for explaination of tf.reduce_max and how axis collapses and stuff**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7w2duU_-psS5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HvHe14VYgfi-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_time_steps = 100\n",
        "num_output_steps = 10\n",
        "dimensions_rnn_cell = 100\n",
        "batch_size = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NSy6Z9ul_hCk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tUlstKfuiH_l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32,shape = [None,num_time_steps,len(vocab_list)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lg6xJZRGATVd"
      },
      "cell_type": "markdown",
      "source": [
        "** Right now too tired, not initializing with Embedding Matrix as its a pain , have no clue what the fuck the function wants from me, come back later here if the performance is not so good , see the documentation for initializer in LSTMCell**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NpXY4lXuX8JI",
        "outputId": "5189dd77-b00d-4e60-9e76-958b5458da31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "lstm_fw_cell = [tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)]\n",
        "lstm_bw_cell = [tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)]\n",
        "\n",
        "outputs, output_state_fw, output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, X, dtype='float32', sequence_length = length(X))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-b83edb18a159>:1: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bXwjpyP4DcLb"
      },
      "cell_type": "markdown",
      "source": [
        "** Cool so session crashes after using all the RAM**\n",
        "\n",
        "** For testing if this is working or not **"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aBs4xL9Sy6Vs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  batch_x, v = next_batch(articles,headings,batch_size)\n",
        "  a1 = sess.run(outputs,feed_dict = {X:batch_x})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "njEL3Nz0y8TD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "8fa2a6d7-20b4-4ed7-d7e6-6b76c8f5f5aa"
      },
      "cell_type": "code",
      "source": [
        "a1.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-7528093c7bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'a1' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XEsTD4P27-QQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Attention Mechanism : For now, wont be taking input from the final output of the network and lets see how it performs, afterwards take as input from the output of the final stage too \n",
        "\n",
        "## I think thats possible because the final outputs are also placeholders right? So that can be input to your 'neural_network' function\n",
        "\n",
        "### Define a Neural Network basically 200 : 100 : 84"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FTQlXl13vWAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "combined_BiRNN_outputs = tf.placeholder(tf.float32,shape = [None,num_time_steps,2*dimensions_rnn_cell])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ANT71WCevWAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_hidden_neurons = 100\n",
        "def neural_network(combined_BiRnn_output):\n",
        "    \n",
        "    Wx1 = tf.Variable(tf.random_normal([batch_size,2*dimensions_rnn_cell,num_hidden_neurons],0.3,1/200))\n",
        "    Wx2 = tf.Variable(tf.random_normal([batch_size,num_hidden_neurons,num_output_steps],0.5,1/50))\n",
        "    b1 = tf.Variable(tf.constant(0.5,shape = [num_hidden_neurons]))\n",
        "    b2 = tf.Variable(tf.constant(0.3,shape = [num_output_steps]))\n",
        "    first_out = tf.nn.tanh(tf.add(tf.matmul(combined_BiRnn_output,Wx1),b1))\n",
        "    e = tf.nn.relu(tf.add(tf.matmul(first_out,Wx2),b2))\n",
        "        \n",
        "    return e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6VQDdOravWAZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(e_values):\n",
        "    j = tf.exp(e_values)\n",
        "    totals = tf.reduce_sum(j,1)\n",
        "    j = tf.unstack(j,axis = 0)\n",
        "    totals = tf.unstack(totals, axis = 0)\n",
        "    for i in range(batch_size):\n",
        "        j[i] = j[i]/totals[i]\n",
        "    j = tf.stack(j,axis = 0)\n",
        "    return j"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DlE0DQWqvWAc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "e_vals = neural_network(combined_BiRNN_outputs)\n",
        "alpha_values = softmax(e_vals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rkN_bf17vWAi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fVD3ollgvWAk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** To test whether alpha values are coming or not **"
      ]
    },
    {
      "metadata": {
        "id": "qeu9WinxvWAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    batch_x, _ = next_batch(articles,headings,batch_size)\n",
        "    outs = sess.run(outputs,feed_dict = {X: batch_x})\n",
        "    \n",
        "    alpha = sess.run(alpha_values,feed_dict = {combined_BiRNN_outputs:outs})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0d2ZC8X_vWAp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Weighted Average **"
      ]
    },
    {
      "metadata": {
        "id": "3H73NLW_vWAr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weighted_average(outputs,alphas):\n",
        "    \n",
        "    alphas = tf.transpose(alphas,perm = [0,2,1])\n",
        "    context_vectors = tf.matmul(alphas,outputs)\n",
        "    \n",
        "    return context_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UuuK1tQtvWAv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "context_vectors = weighted_average(combined_BiRNN_outputs,alpha_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WJ7b1tolvWA1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Another idea you could do is instead of one hot encoding the input words use the embedding matrix you created, though wont be as useful if you could load up the embedding matrix as the weight vector it may be better than normal one hot encoding\n",
        "\n",
        "### Or use a LSTM cell as the first layer and top it off with 2 BiRnn layers or something if LSTM cell supports initialzing \n"
      ]
    },
    {
      "metadata": {
        "id": "Vhu5DFy1vWA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "context = tf.placeholder(tf.float32,shape = [batch_size,num_output_steps,2*dimensions_rnn_cell])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZGKxTECavWBD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cell_factory = tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell,activation=tf.nn.relu)\n",
        "initial_state = cell_factory.zero_state(batch_size, dtype=tf.float32)\n",
        "decoder, _ = tf.nn.dynamic_rnn(cell_factory,context,initial_state=initial_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKDYU-NLvWBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def output_neural_network(decoder_rnn):\n",
        "    \n",
        "    Wx1 = tf.Variable(tf.random_normal([batch_size,dimensions_rnn_cell,len(vocab_list)],0.5,1/dimensions_rnn_cell))\n",
        "    b1 = tf.Variable(tf.constant(0.5,shape = [len(vocab_list)]))\n",
        "    first_out = tf.nn.softmax(tf.nn.relu((tf.add(tf.matmul(decoder_rnn,Wx1),b1))))\n",
        "        \n",
        "    return first_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fSpDOiUbvWBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "probability_distribution = output_neural_network(decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_az7h5NSxPet",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_true = tf.placeholder(tf.float32,shape = [None,num_output_steps,len(vocab_list)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqQ8dtuOvWBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c_e = tf.nn.softmax_cross_entropy_with_logits_v2(logits = probability_distribution,labels = y_true)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = 0.3)\n",
        "train = optimizer.minimize(c_e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v7KyPVBWvWBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "4d1203b9-c547-44d2-9df7-00ce4c3bf6d5"
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for i in range(100):\n",
        "    batch_x , batch_y = next_batch(articles,headings,batch_size,num_time_steps,num_output_steps)\n",
        "    outs = sess.run(outputs, feed_dict = {X:batch_x})\n",
        "    con = sess.run(context_vectors, feed_dict = {combined_BiRNN_outputs:outs})\n",
        "    sess.run(train, feed_dict = {context:con,y_true:batch_y})\n",
        "    \n",
        "    if i%10 ==0:\n",
        "      print(sess.run(c_e,feed_dict = {context:con,y_true:batch_y}))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[12.017939 12.017939 12.017939 12.017939 12.017939 12.017939 12.017939\n",
            "  12.017939 12.017939 12.017939]\n",
            " [12.017939 12.017939 12.017939 12.017939 12.017939 12.017939 12.017939\n",
            "  12.017939 12.017939 12.017939]]\n",
            "[[12.017937 12.017938 12.017872 12.017664 12.014348 11.580312 11.017958\n",
            "  11.01795  11.01795  11.01795 ]\n",
            " [12.01794  12.017938 12.017934 12.017931 12.017938 12.01791  12.017931\n",
            "  12.017949 12.017952 12.01795 ]]\n",
            "[[12.017907 12.01795  12.01795  12.01795  12.01795  12.01795  12.01795\n",
            "  12.01795  12.01795  12.01795 ]\n",
            " [12.017936 12.01795  12.01795  12.01795  11.01795  12.01795  12.01795\n",
            "  12.01795  12.01795  12.01795 ]]\n",
            "[[12.017951 12.01795  12.01795  12.01795  12.01795  12.01795  12.01795\n",
            "  12.01795  12.01795  12.01795 ]\n",
            " [12.017947 12.01795  12.01795  12.01795  12.01795  12.01795  12.01795\n",
            "  12.01795  12.01795  12.01795 ]]\n",
            "[[11.018006 12.01795  12.01795  12.01795  12.01795  12.01795  12.01795\n",
            "  12.01795  12.01795  11.01795 ]\n",
            " [12.017952 12.01795  12.01795  12.01795  12.01795  12.01795  12.01795\n",
            "  12.01795  12.01795  12.01795 ]]\n",
            "[[12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795\n",
            "  12.01795 12.01795]\n",
            " [12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795\n",
            "  12.01795 12.01795]]\n",
            "[[12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795\n",
            "  11.01795 11.01795]\n",
            " [12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795\n",
            "  12.01795 12.01795]]\n",
            "[[12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 11.01795\n",
            "  11.01795 11.01795]\n",
            " [12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795\n",
            "  12.01795 12.01795]]\n",
            "[[12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795\n",
            "  11.01795 11.01795]\n",
            " [12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795\n",
            "  12.01795 12.01795]]\n",
            "[[12.01795 12.01795 12.01795 12.01795 12.01795 11.01795 11.01795 11.01795\n",
            "  11.01795 11.01795]\n",
            " [12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795 12.01795\n",
            "  12.01795 12.01795]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wDaBB_NLyrLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}