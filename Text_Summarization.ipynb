{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvTzv024CFxE"
   },
   "source": [
    "# Get BBC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-Ss8zFVjCxhN",
    "outputId": "d1d037fe-58ba-4e42-95c4-815ef8e7bbf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at ./gdrive; to attempt to forcibly remount, call drive.mount(\"./gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"./gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "X33wMiJNDV5U",
    "outputId": "9de97d94-d650-442b-e294-ab52ae15b606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/Datasets/Text Summarization\n"
     ]
    }
   ],
   "source": [
    "cd gdrive/My\\ Drive/Colab\\ Notebooks/Datasets/Text\\ Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXmwymrPDflS"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('myfile.pkl','rb') as fp:\n",
    "  heading, article = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szyhdhBkEVgi"
   },
   "source": [
    "**Data from BBC News Dataset Loaded. The variables are *heading* and *article***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6eaGtlz-E3Yx"
   },
   "source": [
    "# Preprocess the Data , remove the \\n\\n and \\xa0 , lower case everything  (stemming/lemmatization advantageous? Try it out later)\n",
    "\n",
    "## I think I should keep the comma's in for now , maybe the Attention mechanism later would find words near a comma a good place to give attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVg5fSryGfAF"
   },
   "outputs": [],
   "source": [
    "heading = [h.lower() for h in heading]\n",
    "article = [a.lower() for a in article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wC_iQj2GmP0"
   },
   "outputs": [],
   "source": [
    "article = [a.replace('\\n','').replace('\\xa0',' ') for a in article]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDC4tH40I-2C"
   },
   "source": [
    "# Toeknize the data , Make Word Embeddings from GloVe\n",
    "\n",
    "## Mainly what I have to do is make a matrix of size (vocab_size, 100) which I can initialize as my weight matrix. Be careful, for now I am assuming no new words can appear in a text other than the ones I have seen before , so later if this works improve it to take in other words too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4THBv0WJF6k"
   },
   "outputs": [],
   "source": [
    "word_dimensions = 100\n",
    "import pandas as pd\n",
    "df = pd.read_csv('glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nE9viBdPNO1r"
   },
   "outputs": [],
   "source": [
    "glove = {key: val.values for key, val in df.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1YyKHeQFjJ0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3kYN7skNU2G"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "for a in article:\n",
    "  tokenized_text = tokenizer.tokenize(a)\n",
    "  for tokens in tokenized_text:\n",
    "    words.append(tokens)\n",
    "    \n",
    "for h in heading:\n",
    "  tokenized_text = tokenizer.tokenize(h)\n",
    "  for tokens in tokenized_text:\n",
    "    words.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QK1w4y1OAW0"
   },
   "outputs": [],
   "source": [
    "wordcounts = Counter(words).most_common(170000)\n",
    "vocab_list = np.array([word for word,_ in wordcounts])\n",
    "dictionary_word_code = {word:code for code,word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2822
    },
    "colab_type": "code",
    "id": "uDvTMhHeBgNy",
    "outputId": "8439fba4-0c24-4c64-b9e6-3f27e41efc1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n"
     ]
    }
   ],
   "source": [
    "embedding_list = []\n",
    "i = 0\n",
    "while i != vocab_list.shape[0] :\n",
    "  try:\n",
    "    embedding_list.append(glove[vocab_list[i]])\n",
    "    i = i+1\n",
    "  except KeyError as e:\n",
    "    embedding_list.append(np.random.normal(0,0.1,[word_dimensions]))\n",
    "    i = i+1\n",
    "    \n",
    "  if i%1000 == 0:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psAAqFnZX6w_"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.vstack(embedding_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XY8MVbKYsGg"
   },
   "source": [
    "**Embedding Matrix is created and in the variable *embedding_matrix***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSQyd9qvY_wT"
   },
   "source": [
    "# Making a Bi-Directional RNN with Attention , no dropout layer\n",
    "\n",
    "## For now making a 3 Layer LSTM deep network (could cause vanishing gradient problem) and not using the EOS thing, will make the summary a fixed number of words , mostly wont be coherent but lets see if any output comes at all first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hh-3a2AHX8U7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE4eDmUgA6Vz"
   },
   "source": [
    "#### 'The' also has a code of 0 and you are gonna zero pad things so could lead to problems\n",
    "\n",
    "#### Also you are using too much RAM afterwards change variable names so you overwriting them and not keep making new ones\n",
    "Ok so for saving RAM I am thinking I will make a pickle file for all the important stuff and load it in so all the useless variables dont take up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkCdPTRG1b5I"
   },
   "outputs": [],
   "source": [
    "tokenized_articles = []\n",
    "tokenized_headings = []\n",
    "for a in article:\n",
    "  tokens = tokenizer.tokenize(a)\n",
    "  tokenized_articles.append(tokens)\n",
    "for h in heading:\n",
    "  tokens = tokenizer.tokenize(h)\n",
    "  tokenized_headings.append(tokens)\n",
    "  \n",
    "max_length_article = max([len(a) for a in tokenized_articles])\n",
    "max_length_heading = max([len(h) for h in tokenized_headings])\n",
    "\n",
    "numerical_articles = []\n",
    "numerical_headings = []\n",
    "for a in tokenized_articles:\n",
    "    list1 = [dictionary_word_code[b] for b in a]\n",
    "    numerical_articles.append(list1)\n",
    "for h in tokenized_headings:\n",
    "  list1 = [dictionary_word_code[b] for b in h]\n",
    "  numerical_headings.append(list1)\n",
    "  \n",
    "padded_numerical_articles = []\n",
    "padded_numerical_headings = []\n",
    "for a in numerical_articles:\n",
    "  zeros = [0]*(max_length_article - len(a))\n",
    "  list1 = a + zeros\n",
    "  padded_numerical_articles.append(list1)\n",
    "for h in numerical_headings:\n",
    "  zeros = [0]*(max_length_heading - len(h))\n",
    "  list1 = h + zeros\n",
    "  padded_numerical_headings.append(list1)\n",
    "  \n",
    "with open(\"input_data.pickle\", \"wb\") as f:\n",
    "    pickle.dump((embedding_matrix,padded_numerical_articles,padded_numerical_headings,vocab_list), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0eeO97yHjAp"
   },
   "source": [
    "# Too much RAM was used so start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nfbmW_XAPL8"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(\"input_data.pickle\",\"rb\") as f:\n",
    "  _ , articles , headings, vocab_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13xfVE8i54mR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def next_batch(articles,headings,batch_size,num_time_steps = 850,num_output_steps = 84):\n",
    "  \n",
    "  snipped_articles = []\n",
    "  snipped_headings = []\n",
    "  \n",
    "  for a in articles:\n",
    "    part_article = a[0:num_time_steps]\n",
    "    snipped_articles.append(part_article)\n",
    "    \n",
    "  random = np.random.randint(0,len(articles) - batch_size -1)\n",
    "  batch_x = np.array(snipped_articles[random:random+batch_size]).reshape([batch_size,num_time_steps,1])\n",
    "  onehotlistarticles = []\n",
    "  for article in batch_x:\n",
    "    for num in article:\n",
    "      zeros = np.array([0]*len(vocab_list))\n",
    "      zeros[num] = 1\n",
    "      onehotlistarticles.append(zeros)\n",
    "  onehotlistarticles = np.array(onehotlistarticles).reshape([batch_size,num_time_steps,-1])\n",
    "  \n",
    "  for h in headings:\n",
    "    part_heading = h[0:num_output_steps]\n",
    "    snipped_headings.append(part_heading)\n",
    "  \n",
    "  batch_y = np.array(snipped_headings[random:random+batch_size]).reshape([batch_size,num_output_steps,1])\n",
    "  onehotlistheadings = []\n",
    "  for heading in batch_y:\n",
    "    for num in heading:\n",
    "      zeros = np.array([0]*len(vocab_list))\n",
    "      zeros[num] = 1\n",
    "      onehotlistheadings.append(zeros)\n",
    "  onehotlistheadings = np.array(onehotlistheadings).reshape([batch_size,num_output_steps,-1])\n",
    "  \n",
    "  return onehotlistarticles , onehotlistheadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qhn7_KS4xxsK"
   },
   "source": [
    "# TensorFlow Graph \n",
    "\n",
    "## Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMtm5eFZmJ4_"
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "  used = tf.reduce_max(sequence, 2)\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHspC9d0uAdB"
   },
   "source": [
    "**See Imext Notebook for explaination of tf.reduce_max and how axis collapses and stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7w2duU_-psS5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvHe14VYgfi-"
   },
   "outputs": [],
   "source": [
    "num_time_steps = 850\n",
    "num_output_steps = 84\n",
    "dimensions_rnn_cell = 100\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSy6Z9ul_hCk"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUlstKfuiH_l"
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,shape = [None,num_time_steps,len(vocab_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lg6xJZRGATVd"
   },
   "source": [
    "** Right now too tired, not initializing with Embedding Matrix as its a pain , have no clue what the fuck the function wants from me, come back later here if the performance is not so good , see the documentation for initializer in LSTMCell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "NpXY4lXuX8JI",
    "outputId": "0395b8e0-1221-4f18-8f39-47df6232a3ae"
   },
   "outputs": [],
   "source": [
    "lstm_fw_cell = [tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell),tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell),tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)]\n",
    "lstm_bw_cell = [tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell),tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell),tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)]\n",
    "\n",
    "outputs, output_state_fw, output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, X, dtype='float32', sequence_length = length(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXwjpyP4DcLb"
   },
   "source": [
    "** Cool so session crashes after using all the RAM**\n",
    "\n",
    "** For testing if this is working or not **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBs4xL9Sy6Vs"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  batch_x, v = next_batch(articles,headings,batch_size)\n",
    "  a1 = sess.run(outputs,feed_dict = {X:batch_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njEL3Nz0y8TD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 850, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "XEsTD4P27-QQ",
    "outputId": "eff37ce1-2e38-4e8f-b656-2ea2b44e3ae8"
   },
   "source": [
    "## Attention Mechanism : For now, wont be taking input from the final output of the network and lets see how it performs, afterwards take as input from the output of the final stage too \n",
    "\n",
    "## I think thats possible because the final outputs are also placeholders right? So that can be input to your 'neural_network' function\n",
    "\n",
    "### Define a Neural Network basically 100 : 50 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_BiRNN_outputs = tf.placeholder(tf.float32,shape = [None,num_time_steps,2*dimensions_rnn_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(combined_BiRnn_output):\n",
    "    \n",
    "    Wx1 = tf.Variable(tf.random_normal([batch_size,200,50],0.3,1/200))\n",
    "    Wx2 = tf.Variable(tf.random_normal([batch_size,50,1],0.5,1/50))\n",
    "    b1 = tf.Variable(tf.constant(0.5,shape = [50]))\n",
    "    b2 = tf.Variable(tf.constant(0.3,shape = [1]))\n",
    "    first_out = tf.nn.tanh(tf.add(tf.matmul(combined_BiRnn_output,Wx1),b1))\n",
    "    e = tf.nn.relu(tf.add(tf.matmul(first_out,Wx2),b2))\n",
    "        \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(e_values):\n",
    "    alphas = tf.exp(e_values)\n",
    "    totals = tf.reduce_sum(tf.exp(e_values),1)\n",
    "    alphas = tf.unstack(alphas,axis = 0)\n",
    "    totals = tf.unstack(totals, axis = 0)\n",
    "    for i in range(batch_size):\n",
    "        alphas[i] = alphas[i]/totals[i]\n",
    "    alphas = tf.stack(alphas,axis = 0)\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_vals = neural_network(combined_BiRNN_outputs)\n",
    "alpha_values = softmax(e_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** To test whether alpha values are coming or not **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_x, v = next_batch(articles,headings,batch_size)\n",
    "    outs = sess.run(outputs,feed_dict = {X: batch_x})\n",
    "    \n",
    "    alpha = sess.run(alpha_values,feed_dict = {combined_BiRNN_outputs:outs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Weighted Average **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-98170c4d09d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweighted_average\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "weighted_average = outputs*alpha_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Summarization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
