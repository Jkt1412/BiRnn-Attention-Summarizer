{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvTzv024CFxE"
   },
   "source": [
    "# Get BBC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28811,
     "status": "ok",
     "timestamp": 1553949324231,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "-Ss8zFVjCxhN",
    "outputId": "4ff2f431-6d77-4c40-c4e9-8ccce4eb02cb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"./gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1553949326682,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "X33wMiJNDV5U",
    "outputId": "c4f407ce-53be-4a0c-e0e4-a40210a4c80f"
   },
   "outputs": [],
   "source": [
    "cd gdrive/My\\ Drive/Colab\\ Notebooks/Datasets/Text\\ Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7649,
     "status": "ok",
     "timestamp": 1553948600015,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "XFb-bEcejbtG",
    "outputId": "fd38894f-45af-4421-d48b-9b4b307edc44"
   },
   "outputs": [],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXmwymrPDflS"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('myfile.pkl','rb') as fp:\n",
    "  heading, article = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1553208395273,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "NzWaAh-ihQA8",
    "outputId": "cbcbd2f4-626f-42f6-ec50-b3314fd63f6b"
   },
   "outputs": [],
   "source": [
    "leny=[]\n",
    "for i in range(19000):\n",
    "    if len(article[i]) < 351:\n",
    "        leny.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1057,
     "status": "ok",
     "timestamp": 1553208390522,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "-ouKCPCWbw1b",
    "outputId": "4ee33be6-3eb9-4514-ca1d-08514cf8f9b8"
   },
   "outputs": [],
   "source": [
    "leny = leny[0:10]\n",
    "short_article = []\n",
    "short_heading = []\n",
    "for i in leny:\n",
    "    short_article.append(article[i])\n",
    "    short_heading.append(heading[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_article[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szyhdhBkEVgi"
   },
   "source": [
    "**Data from BBC News Dataset Loaded. The variables are *heading* and *article***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6eaGtlz-E3Yx"
   },
   "source": [
    "# Preprocess the Data , remove the \\n\\n and \\xa0 , lower case everything  (stemming/lemmatization advantageous? Try it out later)\n",
    "\n",
    "## I think I should keep the comma's in for now , maybe the Attention mechanism later would find words near a comma a good place to give attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVg5fSryGfAF"
   },
   "outputs": [],
   "source": [
    "short_heading = [h.lower() for h in short_heading]\n",
    "short_article = [a.lower() for a in short_article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wC_iQj2GmP0"
   },
   "outputs": [],
   "source": [
    "short_article = [a.replace('\\n','').replace('\\xa0',' ').replace('\\r','').replace('\\t','') for a in short_article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1553208448156,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "Uj4Eii5gb-yy",
    "outputId": "9dadce87-317e-4f0f-9885-ba29afa82e93"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess(reviews):\n",
    "  reviews = [REPLACE_NO_SPACE.sub(\"\",line.lower()) for line in reviews]\n",
    "  reviews = [REPLACE_WITH_SPACE.sub(\" \",line) for line in reviews]\n",
    "  \n",
    "  return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZsunLje_TdH"
   },
   "outputs": [],
   "source": [
    "short_articles = preprocess(short_article)\n",
    "short_headings = preprocess(short_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1283,
     "status": "ok",
     "timestamp": 1553208458769,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "MW_j9LF7cCiT",
    "outputId": "d0d1c783-f64c-4c0a-d78a-f9367bcde256"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDC4tH40I-2C"
   },
   "source": [
    "# Toeknize the data , Make Word Embeddings from GloVe\n",
    "\n",
    "## Mainly what I have to do is make a matrix of size (vocab_size, 100) which I can initialize as my weight matrix. Be careful, for now I am assuming no new words can appear in a text other than the ones I have seen before , so later if this works improve it to take in other words too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4THBv0WJF6k"
   },
   "outputs": [],
   "source": [
    "word_dimensions = 100\n",
    "import pandas as pd\n",
    "df = pd.read_csv('glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nE9viBdPNO1r"
   },
   "outputs": [],
   "source": [
    "glove = {key: val.values for key, val in df.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1YyKHeQFjJ0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3kYN7skNU2G"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "for a in short_articles:\n",
    "  tokenized_text = tokenizer.tokenize(a)\n",
    "  for tokens in tokenized_text:\n",
    "    words.append(tokens)\n",
    "    \n",
    "for h in short_headings:\n",
    "  tokenized_text = tokenizer.tokenize(h)\n",
    "  for tokens in tokenized_text:\n",
    "    words.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1318,
     "status": "ok",
     "timestamp": 1553758535038,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "5SxM19TaOaDd",
    "outputId": "f5275f54-4998-43c7-fecc-9b5105dd972b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QK1w4y1OAW0"
   },
   "outputs": [],
   "source": [
    "wordcounts = Counter(words).most_common(500)\n",
    "vocab_list = np.array([word for word,_ in wordcounts])\n",
    "dictionary_word_code = {word:code for code,word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2822
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22305,
     "status": "ok",
     "timestamp": 1553208517886,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "uDvTMhHeBgNy",
    "outputId": "a1107068-4a66-4221-fa87-07fd00eb2792"
   },
   "outputs": [],
   "source": [
    "embedding_list = []\n",
    "i = 0\n",
    "while i != vocab_list.shape[0] :\n",
    "  try:\n",
    "    embedding_list.append(glove[vocab_list[i]])\n",
    "    i = i+1\n",
    "  except KeyError as e:\n",
    "    embedding_list.append(np.random.normal(0,0.1,[word_dimensions]))\n",
    "    i = i+1\n",
    "    \n",
    "  if i%100 == 0:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psAAqFnZX6w_"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.vstack(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1080,
     "status": "ok",
     "timestamp": 1553208540707,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "K_L70jolcWA7",
    "outputId": "84d398ba-b25d-4ac2-955a-68131b5f8147"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XY8MVbKYsGg"
   },
   "source": [
    "**Embedding Matrix is created and in the variable *embedding_matrix***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSQyd9qvY_wT"
   },
   "source": [
    "# Making a Bi-Directional RNN with Attention , no dropout layer\n",
    "\n",
    "## For now making a 3 Layer LSTM deep network (could cause vanishing gradient problem) and not using the EOS thing, will make the summary a fixed number of words , mostly wont be coherent but lets see if any output comes at all first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hh-3a2AHX8U7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE4eDmUgA6Vz"
   },
   "source": [
    "#### 'The' also has a code of 0 and you are gonna zero pad things so could lead to problems\n",
    "\n",
    "#### Also you are using too much RAM afterwards change variable names so you overwriting them and not keep making new ones\n",
    "Ok so for saving RAM I am thinking I will make a pickle file for all the important stuff and load it in so all the useless variables dont take up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkCdPTRG1b5I"
   },
   "outputs": [],
   "source": [
    "tokenized_articles = []\n",
    "tokenized_headings = []\n",
    "for a in short_articles:\n",
    "  tokens = tokenizer.tokenize(a)\n",
    "  tokenized_articles.append(tokens)\n",
    "for h in short_headings:\n",
    "  tokens = tokenizer.tokenize(h)\n",
    "  tokenized_headings.append(tokens)\n",
    "  \n",
    "max_length_article = max([len(a) for a in tokenized_articles])\n",
    "max_length_heading = max([len(h) for h in tokenized_headings])\n",
    "\n",
    "numerical_articles = []\n",
    "numerical_headings = []\n",
    "for a in tokenized_articles:\n",
    "    list1 = [dictionary_word_code[b] for b in a]\n",
    "    numerical_articles.append(list1)\n",
    "for h in tokenized_headings:\n",
    "  list1 = [dictionary_word_code[b] for b in h]\n",
    "  numerical_headings.append(list1)\n",
    "  \n",
    "padded_numerical_articles = []\n",
    "padded_numerical_headings = []\n",
    "for a in numerical_articles:\n",
    "  zeros = [0]*(max_length_article - len(a))\n",
    "  list1 = a + zeros\n",
    "  padded_numerical_articles.append(list1)\n",
    "for h in numerical_headings:\n",
    "  zeros = [0]*(max_length_heading - len(h))\n",
    "  list1 = h + zeros\n",
    "  padded_numerical_headings.append(list1)\n",
    "  \n",
    "with open(\"input_data.pickle\", \"wb\") as f:\n",
    "    pickle.dump((embedding_matrix,padded_numerical_articles,padded_numerical_headings,vocab_list), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0eeO97yHjAp"
   },
   "source": [
    "# Too much RAM was used so start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nfbmW_XAPL8"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(\"input_data.pickle\",\"rb\") as f:\n",
    "  embedding_matrix , articles , headings, vocab_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[283, 284, 285, 3, 286, 3, 0, 287, 24, 45, 16, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headings[1][0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings[4][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[238]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13xfVE8i54mR"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def next_batch(articles,headings,batch_size,num_time_steps,num_output_steps):                 #850     #84\n",
    "  \n",
    "#   snipped_articles = []\n",
    "#   snipped_headings = []\n",
    "  \n",
    "#   for a in articles:\n",
    "#     part_article = a[0:num_time_steps]\n",
    "#     snipped_articles.append(part_article)\n",
    "    \n",
    "#   random = np.random.randint(0,len(articles) - batch_size -1)\n",
    "#   batch_x = np.array(snipped_articles[random:random+batch_size]).reshape([batch_size,num_time_steps,1])\n",
    "#   onehotlistarticles = []\n",
    "# #  for article in batch_x:\n",
    "# #    for num in article:\n",
    "# #      zeros = np.array([0]*len(vocab_list))\n",
    "# #      zeros[num] = 1\n",
    "# #      onehotlistarticles.append(zeros)\n",
    "# #  onehotlistarticles = np.array(onehotlistarticles).reshape([batch_size,num_time_steps,-1])\n",
    "  \n",
    "#   for h in headings:\n",
    "#     part_heading = h[0:num_output_steps]\n",
    "#     snipped_headings.append(part_heading)\n",
    "  \n",
    "#   batch_y = np.array(snipped_headings[random:random+batch_size]).reshape([batch_size,num_output_steps,1])\n",
    "#   onehotlistheadings = []\n",
    "#   for heading in batch_y:\n",
    "#     for num in heading:\n",
    "#       zeros = np.array([0]*len(vocab_list))\n",
    "#       zeros[num] = 1\n",
    "#       onehotlistheadings.append(zeros)\n",
    "#   onehotlistheadings = np.array(onehotlistheadings).reshape([batch_size,num_output_steps,-1])\n",
    "  \n",
    "#   return onehotlistarticles , onehotlistheadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def next_batch(articles,headings,batch_size,num_time_steps,num_output_steps):                 #850     #84\n",
    "  \n",
    "  snipped_articles = []\n",
    "  snipped_headings = []\n",
    "  \n",
    "  for a in articles:\n",
    "    part_article = a[0:num_time_steps]\n",
    "    snipped_articles.append(part_article)\n",
    "    \n",
    "  random = np.random.randint(0,len(articles) - batch_size -1)\n",
    "  batch_x = np.array(snipped_articles[random:random+batch_size]).reshape([batch_size,num_time_steps,1])\n",
    "  #batch_x = np.array([snipped_articles[2],snipped_articles[4]]).reshape([batch_size,num_time_steps,1])\n",
    "  embeddedarticles = []\n",
    "  for article in batch_x:\n",
    "    for num in article:\n",
    "        value = embedding_matrix[num]\n",
    "        embeddedarticles.append(value)\n",
    "  \n",
    "  embeddedarticles = np.array(embeddedarticles).reshape([batch_size,num_time_steps,-1])\n",
    "  \n",
    "  for h in headings:\n",
    "    part_heading = h[0:num_output_steps]\n",
    "    snipped_headings.append(part_heading)\n",
    "  \n",
    "  batch_y = np.array(snipped_headings[random:random+batch_size]).reshape([batch_size,num_output_steps,1])\n",
    "  #batch_y = np.array([snipped_headings[2],snipped_headings[4]]).reshape([batch_size,num_output_steps,1])\n",
    "  onehotlistheadings = []\n",
    "  for heading in batch_y:\n",
    "    for num in heading:\n",
    "      zeros = np.array([0]*len(vocab_list))\n",
    "      zeros[num] = 1\n",
    "      onehotlistheadings.append(zeros)\n",
    "  onehotlistheadings = np.array(onehotlistheadings).reshape([batch_size,num_output_steps,-1])\n",
    "  \n",
    "  return embeddedarticles , onehotlistheadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsM5HHVZp1AK"
   },
   "outputs": [],
   "source": [
    "onehotencodedarticle , onehotencodedheading = next_batch(articles,headings,5,55,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1553279355165,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "brEdaUprqVIY",
    "outputId": "7b415c55-00c7-44fd-d1f5-e5951021c685"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 55, 100)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotencodedarticle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1553279324869,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "84xf5k1nqUpK",
    "outputId": "293f074a-03bd-486e-d711-3bfba51a2426"
   },
   "outputs": [],
   "source": [
    "type(embedding_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qhn7_KS4xxsK"
   },
   "source": [
    "# TensorFlow Graph \n",
    "\n",
    "## Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMtm5eFZmJ4_"
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "  used = tf.reduce_max(sequence, 2)\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHspC9d0uAdB"
   },
   "source": [
    "**See Imext Notebook for explaination of tf.reduce_max and how axis collapses and stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7w2duU_-psS5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvHe14VYgfi-"
   },
   "outputs": [],
   "source": [
    "num_time_steps = 55            #850\n",
    "num_output_steps = 13            #84\n",
    "dimensions_rnn_cell = 50         #100\n",
    "batch_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSy6Z9ul_hCk"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUlstKfuiH_l"
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,shape = [None,num_time_steps,embedding_matrix.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lg6xJZRGATVd"
   },
   "source": [
    "** Right now too tired, not initializing with Embedding Matrix as its a pain , have no clue what the fuck the function wants from me, come back later here if the performance is not so good , see the documentation for initializer in LSTMCell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7262,
     "status": "ok",
     "timestamp": 1553949466353,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "NpXY4lXuX8JI",
    "outputId": "8274c3e7-b6db-4399-ef53-fe3a38c2c557"
   },
   "outputs": [],
   "source": [
    "lstm_fw_cell = [tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell),tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)] #,tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)]\n",
    "lstm_bw_cell = [tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell),tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)] #,tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)]\n",
    "\n",
    "outputs, output_state_fw, output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, X, dtype='float32', sequence_length = length(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXwjpyP4DcLb"
   },
   "source": [
    "** Cool so session crashes after using all the RAM**\n",
    "\n",
    "** For testing if this is working or not **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBs4xL9Sy6Vs"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  batch_x, v = next_batch(articles,headings,batch_size,100,10)\n",
    "  combined_bi_rnn_output = sess.run(outputs,feed_dict = {X:batch_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1146,
     "status": "ok",
     "timestamp": 1553759327090,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "njEL3Nz0y8TD",
    "outputId": "c2021343-0ed2-441f-a5ec-a811bfab1bdb"
   },
   "outputs": [],
   "source": [
    "combined_bi_rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEsTD4P27-QQ"
   },
   "outputs": [],
   "source": [
    "## Attention Mechanism : For now, wont be taking input from the final output of the network and lets see how it performs, afterwards take as input from the output of the final stage too \n",
    "\n",
    "## I think thats possible because the final outputs are also placeholders right? So that can be input to your 'neural_network' function\n",
    "\n",
    "### Define a Neural Network basically 200 : 100 : 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTQlXl13vWAU"
   },
   "outputs": [],
   "source": [
    "combined_BiRNN_outputs = tf.placeholder(tf.float32,shape = [None,num_time_steps,2*dimensions_rnn_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANT71WCevWAX"
   },
   "outputs": [],
   "source": [
    "num_hidden_neurons = 30\n",
    "def neural_network(combined_BiRnn_output):\n",
    "    \n",
    "    Wx1 = tf.Variable(tf.random_normal([batch_size,2*dimensions_rnn_cell,num_hidden_neurons],1,1/100))\n",
    "    Wx2 = tf.Variable(tf.random_normal([batch_size,num_hidden_neurons,num_output_steps],1,1/30))\n",
    "    b1 = tf.Variable(tf.constant(0.0,shape = [num_hidden_neurons]))\n",
    "    b2 = tf.Variable(tf.constant(0.0,shape = [num_output_steps]))\n",
    "    first_out = tf.nn.tanh(tf.add(tf.matmul(combined_BiRnn_output,Wx1),b1))\n",
    "    e = tf.nn.relu(tf.add(tf.matmul(first_out,Wx2),b2))\n",
    "        \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VQDdOravWAZ"
   },
   "outputs": [],
   "source": [
    "def softmax(e_values):\n",
    "    j = tf.exp(e_values)\n",
    "    totals = tf.reduce_sum(j,1)\n",
    "    j = tf.unstack(j,axis = 0)\n",
    "    totals = tf.unstack(totals, axis = 0)\n",
    "    for i in range(batch_size):\n",
    "        j[i] = j[i]/totals[i]\n",
    "    j = tf.stack(j,axis = 0)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DlE0DQWqvWAc"
   },
   "outputs": [],
   "source": [
    "e_vals = neural_network(combined_BiRNN_outputs)\n",
    "alpha_values = softmax(e_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkN_bf17vWAi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVD3ollgvWAk"
   },
   "source": [
    "** To test whether alpha values are coming or not **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeu9WinxvWAm"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_x, _ = next_batch(articles,headings,batch_size,num_time_steps,num_output_steps)\n",
    "    outs = sess.run(outputs,feed_dict = {X: batch_x})\n",
    "    \n",
    "    alpha = sess.run(alpha_values,feed_dict = {combined_BiRNN_outputs:outs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1124,
     "status": "ok",
     "timestamp": 1553759448339,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "QUmgO-0WR59c",
    "outputId": "9c1022d3-8596-4075-852f-273dde46e6cd"
   },
   "outputs": [],
   "source": [
    "alpha[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0d2ZC8X_vWAp"
   },
   "source": [
    "** Weighted Average **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H73NLW_vWAr"
   },
   "outputs": [],
   "source": [
    "def weighted_average(outputs,alphas):\n",
    "    \n",
    "    alphas = tf.transpose(alphas,perm = [0,2,1])\n",
    "    context_vectors = tf.matmul(alphas,outputs)\n",
    "    \n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuuK1tQtvWAv"
   },
   "outputs": [],
   "source": [
    "context_vectors = weighted_average(combined_BiRNN_outputs,alpha_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ7b1tolvWA1"
   },
   "source": [
    "### Another idea you could do is instead of one hot encoding the input words use the embedding matrix you created, though wont be as useful if you could load up the embedding matrix as the weight vector it may be better than normal one hot encoding\n",
    "\n",
    "### Or use a LSTM cell as the first layer and top it off with 2 BiRnn layers or something if LSTM cell supports initialzing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vhu5DFy1vWA3"
   },
   "outputs": [],
   "source": [
    "context = tf.placeholder(tf.float32,shape = [batch_size,num_output_steps,2*dimensions_rnn_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGKxTECavWBD"
   },
   "outputs": [],
   "source": [
    "cell_factory = tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell,activation=tf.nn.relu)\n",
    "initial_state = cell_factory.zero_state(batch_size, dtype=tf.float32)\n",
    "decoder, _ = tf.nn.dynamic_rnn(cell_factory,context,initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKDYU-NLvWBJ"
   },
   "outputs": [],
   "source": [
    "def output_neural_network(decoder_rnn):\n",
    "    \n",
    "    Wx1 = tf.Variable(tf.random_normal([batch_size,dimensions_rnn_cell,len(vocab_list)],0.5,1/dimensions_rnn_cell))\n",
    "    b1 = tf.Variable(tf.constant(0.0,shape = [len(vocab_list)]))\n",
    "    first_out = tf.nn.relu((tf.add(tf.matmul(decoder_rnn,Wx1),b1)))\n",
    "        \n",
    "    return first_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSpDOiUbvWBO"
   },
   "outputs": [],
   "source": [
    "probability_distribution = output_neural_network(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_az7h5NSxPet"
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32,shape = [None,num_output_steps,len(vocab_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqQ8dtuOvWBT"
   },
   "outputs": [],
   "source": [
    "c_e = tf.nn.softmax_cross_entropy_with_logits_v2(logits = probability_distribution,labels = y_true)\n",
    "c_e_s = tf.reduce_mean(c_e)\n",
    "mse = tf.reduce_mean(tf.square(probability_distribution-y_true))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.002)\n",
    "train = optimizer.minimize(c_e_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdhC7Ih5nDo6"
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x , batch_y = next_batch(articles,headings,batch_size,num_time_steps,num_output_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "v7KyPVBWvWBX",
    "outputId": "16c3166f-543e-4f06-dbf8-408c99e5ef54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy at Step 0 : \n",
      "5.734525\n",
      "\n",
      "\n",
      "Cross entropy at Step 100 : \n",
      "1.5827332\n",
      "\n",
      "\n",
      "Cross entropy at Step 200 : \n",
      "1.0446652\n",
      "\n",
      "\n",
      "Cross entropy at Step 300 : \n",
      "0.5008614\n",
      "\n",
      "\n",
      "Cross entropy at Step 400 : \n",
      "0.24542199\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  #batch_x , batch_y = next_batch(articles,headings,batch_size,num_time_steps,num_output_steps)\n",
    "  for i in range(500):\n",
    "    batch_x , batch_y = next_batch(articles,headings,batch_size,num_time_steps,num_output_steps)\n",
    "    outs = sess.run(outputs, feed_dict = {X:batch_x})\n",
    "    con = sess.run(context_vectors, feed_dict = {combined_BiRNN_outputs:outs})\n",
    "    sess.run(train, feed_dict = {context:con,y_true:batch_y})\n",
    "    \n",
    "    if i%100 == 0:\n",
    "      print('Cross entropy at Step {0} : '.format(i))\n",
    "      print(sess.run(c_e_s,feed_dict = {context:con,y_true:batch_y}))\n",
    "      print(\"\\n\")\n",
    "      \n",
    "  saver.save(sess,'./imext_summarizer_1.ckpt')\n",
    "      \n",
    "    #batch_x , batch_y = next_batch(articles,headings,1,100,10)\n",
    "    #outs = sess.run(outputs, feed_dict = {X:batch_x})\n",
    "    #con = sess.run(context_vectors, feed_dict = {combined_BiRNN_outputs:outs})\n",
    "    #final_output = sess.run(probability_distribution, feed_dict = {context:con})\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1378,
     "status": "ok",
     "timestamp": 1553793564468,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "wDaBB_NLyrLl",
    "outputId": "695708ed-28c7-49a1-f7e0-edeef9abe8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./imext_summarizer_1.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_x , batch_y = next_batch(articles,headings,batch_size,num_time_steps,num_output_steps)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./imext_summarizer_1.ckpt')\n",
    "    \n",
    "    outs = sess.run(outputs, feed_dict = {X:batch_x})\n",
    "    con = sess.run(context_vectors, feed_dict = {combined_BiRNN_outputs:outs})\n",
    "    final_output = sess.run(probability_distribution, feed_dict = {context:con})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 13, 315)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1kIjphCT9vC"
   },
   "outputs": [],
   "source": [
    "numerical_words = np.argmax(final_output[1],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 19,  1,  6, 48, 49,  3, 20,  1, 21,  2, 22, 50])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survivor',\n",
       " 'upset',\n",
       " 'use',\n",
       " 'of',\n",
       " 'eye',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tiger',\n",
       " 'with',\n",
       " 'kim',\n",
       " 'davis',\n",
       " 'the',\n",
       " 'the']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = []\n",
    "for i in numerical_words:\n",
    "    head.append(vocab_list[i])\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark',\n",
       " 'and',\n",
       " 'full',\n",
       " 'bodied',\n",
       " 'does',\n",
       " 'it',\n",
       " 'for',\n",
       " 'me',\n",
       " 'every',\n",
       " 'time',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arti = []\n",
    "for i in headings[4]:\n",
    "    arti.append(vocab_list[i])\n",
    "arti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output[0][0][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Summarization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
