{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvTzv024CFxE"
   },
   "source": [
    "# Get BBC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28811,
     "status": "ok",
     "timestamp": 1553949324231,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "-Ss8zFVjCxhN",
    "outputId": "4ff2f431-6d77-4c40-c4e9-8ccce4eb02cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at ./gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"./gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1553949326682,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "X33wMiJNDV5U",
    "outputId": "c4f407ce-53be-4a0c-e0e4-a40210a4c80f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/Datasets/Text Summarization\n"
     ]
    }
   ],
   "source": [
    "cd gdrive/My\\ Drive/Colab\\ Notebooks/Datasets/Text\\ Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7649,
     "status": "ok",
     "timestamp": 1553948600015,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "XFb-bEcejbtG",
    "outputId": "fd38894f-45af-4421-d48b-9b4b307edc44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 1.13.1\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: opensource@google.com\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.6/dist-packages\n",
      "Requires: tensorboard, absl-py, wheel, six, grpcio, protobuf, keras-applications, tensorflow-estimator, termcolor, gast, numpy, astor, keras-preprocessing\n",
      "Required-by: stable-baselines, magenta, fancyimpute\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXmwymrPDflS"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('myfile.pkl','rb') as fp:\n",
    "  heading, article = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1553208395273,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "NzWaAh-ihQA8",
    "outputId": "cbcbd2f4-626f-42f6-ec50-b3314fd63f6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Product Gives Marketers Access to Real Keywords, Conversions and Results Along With 13 Months of Historical Data \\n\\nSAN FRANCISCO, CA -- (Marketwired) -- 09/17/15 -- Jumpshot, a marketing analytics company that uses distinctive data sources to paint a complete picture of the online customer journey, today announced the launch of Jumpshot Elite, giving marketers insight into what their customers are doing the 99% of the time they\\'re not on your site. For years, marketers have been unable to see what organic and paid search terms users were entering, much less tie those searches to purchases. Jumpshot not only injects that user search visibility back into the market, but also makes it possible to tie those keywords to conversions -- for any web site. \\n\\n\"Ever since search engines encrypted search results, marketers have been in the dark about keywords, impacting not only the insight into their own search investments, but also their ability to unearth high converting keywords for their competitors,\" said Deren Baker, CEO of Jumpshot. \"Our platform eliminates the hacks, assumptions, and guesswork that marketers are doing now and provides real data: actual searches tied to actual conversions conducted by real people with nothing inferred.\" \\n\\nUnlike other keyword research tools that receive data through the Adwords API or send bots to cobble together various data inputs and implied metrics, Jumpshot leverages its panel of over 115 million global consumers to analyze real search activity. As a result, Jumpshot is able to provide companies with actionable data to improve the ROI of their search marketing campaigns, SEO tactics and content marketing initiatives. \\n\\nAvailable today, Jumpshot Elite provides 13 months of backward-looking data as well as: \\n\\nAccess to real queries used by searchers \\n\\nPaid and organic results for any website \\n\\nVisibility into organic keywords, eliminating the \"not provided\" outcome in web analytics \\n\\nReal user queries, clicks and transactions instead of machine-generated clicks with inferred results \\n\\nAbility to tie keywords to real transactions on any website \\n\\nVariable attribution models and lookback windows \\n\\nLaunched in January, 2015, Jumpshot grew out of the ambitions of a group of smart marketers and data scientists who were frustrated about the limitations of the data they had access to, and excited about the opportunity to provide new insights into online behavior. \\n\\nThe company uses distinctive data sources to paint a complete picture of the online world for businesses, from where customers spend time online to what they do there and how they get from place to place. By tracking the online customer journey down to each click, Jumpshot reveals how and why customers arrive at purchase decisions. The company tracks more data in more detail than other services, tracking 160 billion monthly clicks generated by its extensive data panel. \\n\\nAbout Jumpshot \\n\\nJumpshot is a marketing analytics platform that reveals the entire customer journey -- from the key sources of traffic to a site, to browsing and buying behavior on any domain. With a panel of 115 million users, Jumpshot provides marketers with the insight to understand what their customers are doing the 99% of the time they\\'re not on their own site -- a scope of information never before attainable. Jumpshot was founded in 2015 and is headquartered in San Francisco. \\n\\nFor more information, please visit www.jumpshot.com. \\n\\nImage Available: http://www2.marketwire.com/mw/frame_mw?attachid=2889222 \\n\\nKelly Mayes \\n\\nThe Bulleit Group \\n\\n615-200-8845 \\n\\nPublished Sep. 17, 2015 \\n\\nCopyright © 2015 SYS-CON Media, Inc. — All Rights Reserved. \\n\\nSyndicated stories and blog feeds, all rights reserved by the author.'"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1057,
     "status": "ok",
     "timestamp": 1553208390522,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "-ouKCPCWbw1b",
    "outputId": "4ee33be6-3eb9-4514-ca1d-08514cf8f9b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jumpshot Gives Marketers Renewed Visibility Into Paid and Organic Keywords With Launch of Jumpshot Elite'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szyhdhBkEVgi"
   },
   "source": [
    "**Data from BBC News Dataset Loaded. The variables are *heading* and *article***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6eaGtlz-E3Yx"
   },
   "source": [
    "# Preprocess the Data , remove the \\n\\n and \\xa0 , lower case everything  (stemming/lemmatization advantageous? Try it out later)\n",
    "\n",
    "## I think I should keep the comma's in for now , maybe the Attention mechanism later would find words near a comma a good place to give attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVg5fSryGfAF"
   },
   "outputs": [],
   "source": [
    "heading = [h.lower() for h in heading]\n",
    "article = [a.lower() for a in article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wC_iQj2GmP0"
   },
   "outputs": [],
   "source": [
    "article = [a.replace('\\n','').replace('\\xa0',' ') for a in article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1553208448156,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "Uj4Eii5gb-yy",
    "outputId": "9dadce87-317e-4f0f-9885-ba29afa82e93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new product gives marketers access to real keywords, conversions and results along with 13 months of historical data san francisco, ca -- (marketwired) -- 09/17/15 -- jumpshot, a marketing analytics company that uses distinctive data sources to paint a complete picture of the online customer journey, today announced the launch of jumpshot elite, giving marketers insight into what their customers are doing the 99% of the time they\\'re not on your site. for years, marketers have been unable to see what organic and paid search terms users were entering, much less tie those searches to purchases. jumpshot not only injects that user search visibility back into the market, but also makes it possible to tie those keywords to conversions -- for any web site. \"ever since search engines encrypted search results, marketers have been in the dark about keywords, impacting not only the insight into their own search investments, but also their ability to unearth high converting keywords for their competitors,\" said deren baker, ceo of jumpshot. \"our platform eliminates the hacks, assumptions, and guesswork that marketers are doing now and provides real data: actual searches tied to actual conversions conducted by real people with nothing inferred.\" unlike other keyword research tools that receive data through the adwords api or send bots to cobble together various data inputs and implied metrics, jumpshot leverages its panel of over 115 million global consumers to analyze real search activity. as a result, jumpshot is able to provide companies with actionable data to improve the roi of their search marketing campaigns, seo tactics and content marketing initiatives. available today, jumpshot elite provides 13 months of backward-looking data as well as: access to real queries used by searchers paid and organic results for any website visibility into organic keywords, eliminating the \"not provided\" outcome in web analytics real user queries, clicks and transactions instead of machine-generated clicks with inferred results ability to tie keywords to real transactions on any website variable attribution models and lookback windows launched in january, 2015, jumpshot grew out of the ambitions of a group of smart marketers and data scientists who were frustrated about the limitations of the data they had access to, and excited about the opportunity to provide new insights into online behavior. the company uses distinctive data sources to paint a complete picture of the online world for businesses, from where customers spend time online to what they do there and how they get from place to place. by tracking the online customer journey down to each click, jumpshot reveals how and why customers arrive at purchase decisions. the company tracks more data in more detail than other services, tracking 160 billion monthly clicks generated by its extensive data panel. about jumpshot jumpshot is a marketing analytics platform that reveals the entire customer journey -- from the key sources of traffic to a site, to browsing and buying behavior on any domain. with a panel of 115 million users, jumpshot provides marketers with the insight to understand what their customers are doing the 99% of the time they\\'re not on their own site -- a scope of information never before attainable. jumpshot was founded in 2015 and is headquartered in san francisco. for more information, please visit www.jumpshot.com. image available: http://www2.marketwire.com/mw/frame_mw?attachid=2889222 kelly mayes the bulleit group 615-200-8845 published sep. 17, 2015 copyright © 2015 sys-con media, inc. — all rights reserved. syndicated stories and blog feeds, all rights reserved by the author.'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZsunLje_TdH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1283,
     "status": "ok",
     "timestamp": 1553208458769,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "MW_j9LF7cCiT",
    "outputId": "d0d1c783-f64c-4c0a-d78a-f9367bcde256"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumpshot gives marketers renewed visibility into paid and organic keywords with launch of jumpshot elite'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDC4tH40I-2C"
   },
   "source": [
    "# Toeknize the data , Make Word Embeddings from GloVe\n",
    "\n",
    "## Mainly what I have to do is make a matrix of size (vocab_size, 100) which I can initialize as my weight matrix. Be careful, for now I am assuming no new words can appear in a text other than the ones I have seen before , so later if this works improve it to take in other words too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4THBv0WJF6k"
   },
   "outputs": [],
   "source": [
    "word_dimensions = 100\n",
    "import pandas as pd\n",
    "df = pd.read_csv('glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nE9viBdPNO1r"
   },
   "outputs": [],
   "source": [
    "glove = {key: val.values for key, val in df.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1YyKHeQFjJ0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3kYN7skNU2G"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "for a in article:\n",
    "  tokenized_text = tokenizer.tokenize(a)\n",
    "  for tokens in tokenized_text:\n",
    "    words.append(tokens)\n",
    "    \n",
    "for h in heading:\n",
    "  tokenized_text = tokenizer.tokenize(h)\n",
    "  for tokens in tokenized_text:\n",
    "    words.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1318,
     "status": "ok",
     "timestamp": 1553758535038,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "5SxM19TaOaDd",
    "outputId": "f5275f54-4998-43c7-fecc-9b5105dd972b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['veterans',\n",
       " 'saluted',\n",
       " 'worcester',\n",
       " \"'\",\n",
       " 's',\n",
       " 'first',\n",
       " 'ever',\n",
       " 'breakfast',\n",
       " 'club',\n",
       " 'for',\n",
       " 'ex',\n",
       " '-',\n",
       " 'soldiers',\n",
       " 'which',\n",
       " 'won',\n",
       " 'over',\n",
       " 'hearts',\n",
       " ',',\n",
       " 'minds',\n",
       " 'and',\n",
       " 'bellies',\n",
       " '.',\n",
       " 'the',\n",
       " 'worcester',\n",
       " 'breakfast',\n",
       " 'club',\n",
       " 'for',\n",
       " 'hm',\n",
       " 'forces',\n",
       " 'veterans',\n",
       " 'met',\n",
       " 'at',\n",
       " 'the',\n",
       " 'postal',\n",
       " 'order',\n",
       " 'in',\n",
       " 'foregate',\n",
       " 'street',\n",
       " 'at',\n",
       " '10am',\n",
       " 'on',\n",
       " 'saturday',\n",
       " '.',\n",
       " 'the',\n",
       " 'club',\n",
       " 'is',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'allow',\n",
       " 'veterans',\n",
       " 'a',\n",
       " 'place',\n",
       " 'to',\n",
       " 'meet',\n",
       " ',',\n",
       " 'socialise',\n",
       " ',',\n",
       " 'eat',\n",
       " 'and',\n",
       " 'drink',\n",
       " ',',\n",
       " 'giving',\n",
       " 'hunger',\n",
       " 'and',\n",
       " 'loneliness',\n",
       " 'their',\n",
       " 'marching',\n",
       " 'orders',\n",
       " '.',\n",
       " 'father',\n",
       " '-',\n",
       " 'of',\n",
       " '-',\n",
       " 'two',\n",
       " 'dave',\n",
       " 'carney',\n",
       " ',',\n",
       " 'aged',\n",
       " '43',\n",
       " ',',\n",
       " 'of',\n",
       " 'merrimans',\n",
       " 'hill',\n",
       " ',',\n",
       " 'worcester',\n",
       " ',',\n",
       " 'set',\n",
       " 'up',\n",
       " 'the',\n",
       " 'club',\n",
       " 'after',\n",
       " 'being',\n",
       " 'inspired',\n",
       " 'by',\n",
       " 'other',\n",
       " 'similar',\n",
       " 'clubs',\n",
       " 'across',\n",
       " 'the',\n",
       " 'country',\n",
       " '.',\n",
       " 'he',\n",
       " 'said',\n",
       " ':',\n",
       " '\"',\n",
       " 'as',\n",
       " 'you',\n",
       " 'can',\n",
       " 'see',\n",
       " 'from',\n",
       " 'the',\n",
       " 'picture',\n",
       " ',',\n",
       " 'we',\n",
       " 'had',\n",
       " 'a',\n",
       " 'good',\n",
       " 'response',\n",
       " '.',\n",
       " 'five',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " '10',\n",
       " 'that',\n",
       " 'attended',\n",
       " 'said',\n",
       " 'they',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'article',\n",
       " 'in',\n",
       " 'the',\n",
       " 'newspaper',\n",
       " 'and',\n",
       " 'turned',\n",
       " 'up',\n",
       " '.',\n",
       " '\"',\n",
       " 'we',\n",
       " 'even',\n",
       " 'had',\n",
       " 'an',\n",
       " 'old',\n",
       " 'chap',\n",
       " 'travel',\n",
       " 'from',\n",
       " 'droitwich',\n",
       " 'and',\n",
       " 'he',\n",
       " 'was',\n",
       " 'late',\n",
       " 'on',\n",
       " 'parade',\n",
       " 'by',\n",
       " 'three',\n",
       " 'hours',\n",
       " '.',\n",
       " '\"',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'generated',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'interest',\n",
       " 'and',\n",
       " 'i',\n",
       " 'estimate',\n",
       " '(',\n",
       " 'from',\n",
       " 'other',\n",
       " 'veterans',\n",
       " 'who',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'article',\n",
       " ')',\n",
       " 'that',\n",
       " 'next',\n",
       " 'month',\n",
       " \"'\",\n",
       " 's',\n",
       " 'meeting',\n",
       " 'will',\n",
       " 'attract',\n",
       " 'about',\n",
       " '20',\n",
       " 'people',\n",
       " '.',\n",
       " 'onwards',\n",
       " 'and',\n",
       " 'upwards',\n",
       " '.\"',\n",
       " 'he',\n",
       " 'said',\n",
       " 'the',\n",
       " 'management',\n",
       " 'at',\n",
       " 'the',\n",
       " 'pub',\n",
       " 'had',\n",
       " 'been',\n",
       " 'extremely',\n",
       " 'hospitable',\n",
       " 'to',\n",
       " 'them',\n",
       " '.',\n",
       " 'mr',\n",
       " 'carney',\n",
       " 'said',\n",
       " ':',\n",
       " '\"',\n",
       " 'they',\n",
       " 'bent',\n",
       " 'over',\n",
       " 'backwards',\n",
       " 'for',\n",
       " 'us',\n",
       " '.',\n",
       " 'they',\n",
       " 'really',\n",
       " 'looked',\n",
       " 'after',\n",
       " 'us',\n",
       " 'well',\n",
       " '.',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'choice',\n",
       " 'of',\n",
       " 'venue',\n",
       " 'i',\n",
       " 'could',\n",
       " 'have',\n",
       " 'made',\n",
       " '.',\n",
       " 'they',\n",
       " 'even',\n",
       " 'put',\n",
       " \"'\",\n",
       " 'reserved',\n",
       " 'for',\n",
       " 'the',\n",
       " 'armed',\n",
       " 'forces',\n",
       " \"'.\",\n",
       " 'promoted',\n",
       " 'stories',\n",
       " 'the',\n",
       " 'reserve',\n",
       " 'veteran',\n",
       " 'with',\n",
       " 'the',\n",
       " 'royal',\n",
       " 'engineers',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'breakfast',\n",
       " 'club',\n",
       " 'but',\n",
       " 'found',\n",
       " 'the',\n",
       " 'nearest',\n",
       " 'ones',\n",
       " 'were',\n",
       " 'in',\n",
       " 'bromsgrove',\n",
       " 'and',\n",
       " 'gloucester',\n",
       " 'so',\n",
       " 'he',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'set',\n",
       " 'up',\n",
       " 'his',\n",
       " 'own',\n",
       " ',',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'home',\n",
       " '.',\n",
       " 'he',\n",
       " 'was',\n",
       " 'influenced',\n",
       " 'by',\n",
       " 'derek',\n",
       " 'hardman',\n",
       " 'who',\n",
       " 'set',\n",
       " 'up',\n",
       " 'a',\n",
       " 'breakfast',\n",
       " 'club',\n",
       " 'for',\n",
       " 'veterans',\n",
       " 'in',\n",
       " 'hull',\n",
       " 'and',\n",
       " 'andy',\n",
       " 'wilson',\n",
       " 'who',\n",
       " 'set',\n",
       " 'one',\n",
       " 'up',\n",
       " 'in',\n",
       " 'newcastle',\n",
       " '.',\n",
       " 'he',\n",
       " 'said',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'has',\n",
       " 'snowballed',\n",
       " 'and',\n",
       " 'there',\n",
       " 'were',\n",
       " 'now',\n",
       " '70',\n",
       " 'similar',\n",
       " 'clubs',\n",
       " 'across',\n",
       " 'the',\n",
       " 'country',\n",
       " 'and',\n",
       " 'even',\n",
       " 'some',\n",
       " 'in',\n",
       " 'germany',\n",
       " '.',\n",
       " 'mr',\n",
       " 'carney',\n",
       " 'said',\n",
       " 'with',\n",
       " 'many',\n",
       " 'royal',\n",
       " 'british',\n",
       " 'legion',\n",
       " 'clubs',\n",
       " 'closing',\n",
       " 'he',\n",
       " 'wanted',\n",
       " 'veterans',\n",
       " 'and',\n",
       " 'serving',\n",
       " 'personnel',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'they',\n",
       " 'had',\n",
       " 'somewhere',\n",
       " 'they',\n",
       " 'could',\n",
       " 'go',\n",
       " 'for',\n",
       " 'good',\n",
       " 'grub',\n",
       " ',',\n",
       " 'beer',\n",
       " 'and',\n",
       " 'banter',\n",
       " 'to',\n",
       " 'recapture',\n",
       " 'the',\n",
       " 'comradery',\n",
       " 'of',\n",
       " 'being',\n",
       " 'in',\n",
       " 'the',\n",
       " 'forces',\n",
       " '.',\n",
       " 'the',\n",
       " 'postal',\n",
       " 'order',\n",
       " 'was',\n",
       " 'chosen',\n",
       " 'because',\n",
       " 'of',\n",
       " 'its',\n",
       " 'central',\n",
       " 'location',\n",
       " 'and',\n",
       " 'its',\n",
       " 'proximity',\n",
       " 'to',\n",
       " 'the',\n",
       " 'railway',\n",
       " 'station',\n",
       " 'and',\n",
       " 'hotels',\n",
       " 'and',\n",
       " 'reasonably',\n",
       " 'priced',\n",
       " 'food',\n",
       " 'and',\n",
       " 'drink',\n",
       " '.',\n",
       " 'the',\n",
       " 'management',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pub',\n",
       " 'have',\n",
       " 'even',\n",
       " 'given',\n",
       " 'the',\n",
       " 'veterans',\n",
       " 'a',\n",
       " 'designated',\n",
       " 'area',\n",
       " 'within',\n",
       " 'the',\n",
       " 'pub',\n",
       " '.',\n",
       " 'share',\n",
       " 'article',\n",
       " 'the',\n",
       " 'next',\n",
       " 'meeting',\n",
       " 'is',\n",
       " 'at',\n",
       " 'the',\n",
       " 'postal',\n",
       " 'order',\n",
       " 'on',\n",
       " 'saturday',\n",
       " ',',\n",
       " 'october',\n",
       " '3',\n",
       " 'at',\n",
       " '10am',\n",
       " '.',\n",
       " 'the',\n",
       " 'breakfast',\n",
       " 'club',\n",
       " 'meets',\n",
       " 'on',\n",
       " 'the',\n",
       " 'first',\n",
       " 'saturday',\n",
       " 'of',\n",
       " 'each',\n",
       " 'month',\n",
       " 'for',\n",
       " 'those',\n",
       " 'who',\n",
       " 'want',\n",
       " 'to',\n",
       " 'attend',\n",
       " 'in',\n",
       " 'future',\n",
       " '.',\n",
       " 'new',\n",
       " 'product',\n",
       " 'gives',\n",
       " 'marketers',\n",
       " 'access',\n",
       " 'to',\n",
       " 'real',\n",
       " 'keywords',\n",
       " ',',\n",
       " 'conversions',\n",
       " 'and',\n",
       " 'results',\n",
       " 'along',\n",
       " 'with',\n",
       " '13',\n",
       " 'months',\n",
       " 'of',\n",
       " 'historical',\n",
       " 'data',\n",
       " 'san',\n",
       " 'francisco',\n",
       " ',',\n",
       " 'ca',\n",
       " '--',\n",
       " '(',\n",
       " 'marketwired',\n",
       " ')',\n",
       " '--',\n",
       " '09',\n",
       " '/',\n",
       " '17',\n",
       " '/',\n",
       " '15',\n",
       " '--',\n",
       " 'jumpshot',\n",
       " ',',\n",
       " 'a',\n",
       " 'marketing',\n",
       " 'analytics',\n",
       " 'company',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'distinctive',\n",
       " 'data',\n",
       " 'sources',\n",
       " 'to',\n",
       " 'paint',\n",
       " 'a',\n",
       " 'complete',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'the',\n",
       " 'online',\n",
       " 'customer',\n",
       " 'journey',\n",
       " ',',\n",
       " 'today',\n",
       " 'announced',\n",
       " 'the',\n",
       " 'launch',\n",
       " 'of',\n",
       " 'jumpshot',\n",
       " 'elite',\n",
       " ',',\n",
       " 'giving',\n",
       " 'marketers',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'what',\n",
       " 'their',\n",
       " 'customers',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'the',\n",
       " '99',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'time',\n",
       " 'they',\n",
       " \"'\",\n",
       " 're',\n",
       " 'not',\n",
       " 'on',\n",
       " 'your',\n",
       " 'site',\n",
       " '.',\n",
       " 'for',\n",
       " 'years',\n",
       " ',',\n",
       " 'marketers',\n",
       " 'have',\n",
       " 'been',\n",
       " 'unable',\n",
       " 'to',\n",
       " 'see',\n",
       " 'what',\n",
       " 'organic',\n",
       " 'and',\n",
       " 'paid',\n",
       " 'search',\n",
       " 'terms',\n",
       " 'users',\n",
       " 'were',\n",
       " 'entering',\n",
       " ',',\n",
       " 'much',\n",
       " 'less',\n",
       " 'tie',\n",
       " 'those',\n",
       " 'searches',\n",
       " 'to',\n",
       " 'purchases',\n",
       " '.',\n",
       " 'jumpshot',\n",
       " 'not',\n",
       " 'only',\n",
       " 'injects',\n",
       " 'that',\n",
       " 'user',\n",
       " 'search',\n",
       " 'visibility',\n",
       " 'back',\n",
       " 'into',\n",
       " 'the',\n",
       " 'market',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'tie',\n",
       " 'those',\n",
       " 'keywords',\n",
       " 'to',\n",
       " 'conversions',\n",
       " '--',\n",
       " 'for',\n",
       " 'any',\n",
       " 'web',\n",
       " 'site',\n",
       " '.',\n",
       " '\"',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'search',\n",
       " 'engines',\n",
       " 'encrypted',\n",
       " 'search',\n",
       " 'results',\n",
       " ',',\n",
       " 'marketers',\n",
       " 'have',\n",
       " 'been',\n",
       " 'in',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'about',\n",
       " 'keywords',\n",
       " ',',\n",
       " 'impacting',\n",
       " 'not',\n",
       " 'only',\n",
       " 'the',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'their',\n",
       " 'own',\n",
       " 'search',\n",
       " 'investments',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'their',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'unearth',\n",
       " 'high',\n",
       " 'converting',\n",
       " 'keywords',\n",
       " 'for',\n",
       " 'their',\n",
       " 'competitors',\n",
       " ',\"',\n",
       " 'said',\n",
       " 'deren',\n",
       " 'baker',\n",
       " ',',\n",
       " 'ceo',\n",
       " 'of',\n",
       " 'jumpshot',\n",
       " '.',\n",
       " '\"',\n",
       " 'our',\n",
       " 'platform',\n",
       " 'eliminates',\n",
       " 'the',\n",
       " 'hacks',\n",
       " ',',\n",
       " 'assumptions',\n",
       " ',',\n",
       " 'and',\n",
       " 'guesswork',\n",
       " 'that',\n",
       " 'marketers',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'now',\n",
       " 'and',\n",
       " 'provides',\n",
       " 'real',\n",
       " 'data',\n",
       " ':',\n",
       " 'actual',\n",
       " 'searches',\n",
       " 'tied',\n",
       " 'to',\n",
       " 'actual',\n",
       " 'conversions',\n",
       " 'conducted',\n",
       " 'by',\n",
       " 'real',\n",
       " 'people',\n",
       " 'with',\n",
       " 'nothing',\n",
       " 'inferred',\n",
       " '.\"',\n",
       " 'unlike',\n",
       " 'other',\n",
       " 'keyword',\n",
       " 'research',\n",
       " 'tools',\n",
       " 'that',\n",
       " 'receive',\n",
       " 'data',\n",
       " 'through',\n",
       " 'the',\n",
       " 'adwords',\n",
       " 'api',\n",
       " 'or',\n",
       " 'send',\n",
       " 'bots',\n",
       " 'to',\n",
       " 'cobble',\n",
       " 'together',\n",
       " 'various',\n",
       " 'data',\n",
       " 'inputs',\n",
       " 'and',\n",
       " 'implied',\n",
       " 'metrics',\n",
       " ',',\n",
       " 'jumpshot',\n",
       " 'leverages',\n",
       " 'its',\n",
       " 'panel',\n",
       " 'of',\n",
       " 'over',\n",
       " '115',\n",
       " 'million',\n",
       " 'global',\n",
       " 'consumers',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'real',\n",
       " 'search',\n",
       " 'activity',\n",
       " '.',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " ',',\n",
       " 'jumpshot',\n",
       " 'is',\n",
       " 'able',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'companies',\n",
       " 'with',\n",
       " 'actionable',\n",
       " 'data',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'roi',\n",
       " 'of',\n",
       " 'their',\n",
       " 'search',\n",
       " 'marketing',\n",
       " 'campaigns',\n",
       " ',',\n",
       " 'seo',\n",
       " 'tactics',\n",
       " 'and',\n",
       " 'content',\n",
       " 'marketing',\n",
       " 'initiatives',\n",
       " '.',\n",
       " 'available',\n",
       " 'today',\n",
       " ',',\n",
       " 'jumpshot',\n",
       " 'elite',\n",
       " 'provides',\n",
       " '13',\n",
       " 'months',\n",
       " 'of',\n",
       " 'backward',\n",
       " '-',\n",
       " 'looking',\n",
       " 'data',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " ':',\n",
       " 'access',\n",
       " 'to',\n",
       " 'real',\n",
       " 'queries',\n",
       " 'used',\n",
       " 'by',\n",
       " 'searchers',\n",
       " 'paid',\n",
       " 'and',\n",
       " 'organic',\n",
       " 'results',\n",
       " 'for',\n",
       " 'any',\n",
       " 'website',\n",
       " 'visibility',\n",
       " 'into',\n",
       " 'organic',\n",
       " 'keywords',\n",
       " ',',\n",
       " 'eliminating',\n",
       " 'the',\n",
       " '\"',\n",
       " 'not',\n",
       " 'provided',\n",
       " '\"',\n",
       " 'outcome',\n",
       " 'in',\n",
       " 'web',\n",
       " 'analytics',\n",
       " 'real',\n",
       " 'user',\n",
       " 'queries',\n",
       " ',',\n",
       " 'clicks',\n",
       " 'and',\n",
       " 'transactions',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'machine',\n",
       " '-',\n",
       " 'generated',\n",
       " 'clicks',\n",
       " 'with',\n",
       " 'inferred',\n",
       " 'results',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'tie',\n",
       " 'keywords',\n",
       " 'to',\n",
       " 'real',\n",
       " 'transactions',\n",
       " 'on',\n",
       " 'any',\n",
       " 'website',\n",
       " 'variable',\n",
       " 'attribution',\n",
       " 'models',\n",
       " 'and',\n",
       " 'lookback',\n",
       " 'windows',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'january',\n",
       " ',',\n",
       " '2015',\n",
       " ',',\n",
       " 'jumpshot',\n",
       " 'grew',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ambitions',\n",
       " 'of',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'smart',\n",
       " 'marketers',\n",
       " 'and',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'who',\n",
       " 'were',\n",
       " 'frustrated',\n",
       " 'about',\n",
       " 'the',\n",
       " 'limitations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'data',\n",
       " 'they',\n",
       " 'had',\n",
       " 'access',\n",
       " 'to',\n",
       " ',',\n",
       " 'and',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'the',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'new',\n",
       " 'insights',\n",
       " 'into',\n",
       " 'online',\n",
       " 'behavior',\n",
       " '.',\n",
       " 'the',\n",
       " 'company',\n",
       " 'uses',\n",
       " 'distinctive',\n",
       " 'data',\n",
       " 'sources',\n",
       " 'to',\n",
       " 'paint',\n",
       " 'a',\n",
       " 'complete',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'the',\n",
       " 'online',\n",
       " 'world',\n",
       " 'for',\n",
       " 'businesses',\n",
       " ',',\n",
       " 'from',\n",
       " 'where',\n",
       " 'customers',\n",
       " 'spend',\n",
       " 'time',\n",
       " 'online',\n",
       " 'to',\n",
       " 'what',\n",
       " 'they',\n",
       " 'do',\n",
       " 'there',\n",
       " 'and',\n",
       " 'how',\n",
       " 'they',\n",
       " 'get',\n",
       " 'from',\n",
       " 'place',\n",
       " 'to',\n",
       " 'place',\n",
       " '.',\n",
       " 'by',\n",
       " 'tracking',\n",
       " 'the',\n",
       " 'online',\n",
       " 'customer',\n",
       " 'journey',\n",
       " 'down',\n",
       " 'to',\n",
       " 'each',\n",
       " 'click',\n",
       " ',',\n",
       " 'jumpshot',\n",
       " 'reveals',\n",
       " 'how',\n",
       " 'and',\n",
       " 'why',\n",
       " 'customers',\n",
       " 'arrive',\n",
       " 'at',\n",
       " 'purchase',\n",
       " 'decisions',\n",
       " '.',\n",
       " 'the',\n",
       " 'company',\n",
       " 'tracks',\n",
       " 'more',\n",
       " 'data',\n",
       " 'in',\n",
       " 'more',\n",
       " 'detail',\n",
       " 'than',\n",
       " 'other',\n",
       " 'services',\n",
       " ',',\n",
       " 'tracking',\n",
       " '160',\n",
       " 'billion',\n",
       " 'monthly',\n",
       " 'clicks',\n",
       " 'generated',\n",
       " 'by',\n",
       " 'its',\n",
       " 'extensive',\n",
       " 'data',\n",
       " 'panel',\n",
       " '.',\n",
       " 'about',\n",
       " 'jumpshot',\n",
       " 'jumpshot',\n",
       " 'is',\n",
       " 'a',\n",
       " 'marketing',\n",
       " 'analytics',\n",
       " 'platform',\n",
       " 'that',\n",
       " 'reveals',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'customer',\n",
       " 'journey',\n",
       " '--',\n",
       " 'from',\n",
       " 'the',\n",
       " 'key',\n",
       " 'sources',\n",
       " 'of',\n",
       " 'traffic',\n",
       " 'to',\n",
       " 'a',\n",
       " 'site',\n",
       " ',',\n",
       " 'to',\n",
       " 'browsing',\n",
       " 'and',\n",
       " 'buying',\n",
       " 'behavior',\n",
       " 'on',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QK1w4y1OAW0"
   },
   "outputs": [],
   "source": [
    "wordcounts = Counter(words).most_common(170000)\n",
    "vocab_list = np.array([word for word,_ in wordcounts])\n",
    "dictionary_word_code = {word:code for code,word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2822
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22305,
     "status": "ok",
     "timestamp": 1553208517886,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "uDvTMhHeBgNy",
    "outputId": "a1107068-4a66-4221-fa87-07fd00eb2792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n"
     ]
    }
   ],
   "source": [
    "embedding_list = []\n",
    "i = 0\n",
    "while i != vocab_list.shape[0] :\n",
    "  try:\n",
    "    embedding_list.append(glove[vocab_list[i]])\n",
    "    i = i+1\n",
    "  except KeyError as e:\n",
    "    embedding_list.append(np.random.normal(0,0.1,[word_dimensions]))\n",
    "    i = i+1\n",
    "    \n",
    "  if i%1000 == 0:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psAAqFnZX6w_"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.vstack(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1080,
     "status": "ok",
     "timestamp": 1553208540707,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "K_L70jolcWA7",
    "outputId": "84d398ba-b25d-4ac2-955a-68131b5f8147"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.038194  , -0.24487   ,  0.72812   , ..., -0.1459    ,\n",
       "         0.8278    ,  0.27062   ],\n",
       "       [-0.33979   ,  0.20941   ,  0.46348   , ..., -0.23394   ,\n",
       "         0.47298   , -0.028803  ],\n",
       "       [-0.10767   ,  0.11053   ,  0.59812   , ..., -0.83155   ,\n",
       "         0.45293   ,  0.082577  ],\n",
       "       ...,\n",
       "       [-0.06889619, -0.0915261 ,  0.03377779, ..., -0.05064638,\n",
       "        -0.03349665, -0.06075078],\n",
       "       [-0.29514   , -0.88073   , -0.35538   , ..., -0.036788  ,\n",
       "        -0.23563   ,  0.054821  ],\n",
       "       [ 0.017643  , -0.090398  , -0.81834   , ..., -0.19481   ,\n",
       "        -0.87331   ,  0.18943   ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XY8MVbKYsGg"
   },
   "source": [
    "**Embedding Matrix is created and in the variable *embedding_matrix***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSQyd9qvY_wT"
   },
   "source": [
    "# Making a Bi-Directional RNN with Attention , no dropout layer\n",
    "\n",
    "## For now making a 3 Layer LSTM deep network (could cause vanishing gradient problem) and not using the EOS thing, will make the summary a fixed number of words , mostly wont be coherent but lets see if any output comes at all first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hh-3a2AHX8U7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE4eDmUgA6Vz"
   },
   "source": [
    "#### 'The' also has a code of 0 and you are gonna zero pad things so could lead to problems\n",
    "\n",
    "#### Also you are using too much RAM afterwards change variable names so you overwriting them and not keep making new ones\n",
    "Ok so for saving RAM I am thinking I will make a pickle file for all the important stuff and load it in so all the useless variables dont take up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkCdPTRG1b5I"
   },
   "outputs": [],
   "source": [
    "tokenized_articles = []\n",
    "tokenized_headings = []\n",
    "for a in article:\n",
    "  tokens = tokenizer.tokenize(a)\n",
    "  tokenized_articles.append(tokens)\n",
    "for h in heading:\n",
    "  tokens = tokenizer.tokenize(h)\n",
    "  tokenized_headings.append(tokens)\n",
    "  \n",
    "max_length_article = max([len(a) for a in tokenized_articles])\n",
    "max_length_heading = max([len(h) for h in tokenized_headings])\n",
    "\n",
    "numerical_articles = []\n",
    "numerical_headings = []\n",
    "for a in tokenized_articles:\n",
    "    list1 = [dictionary_word_code[b] for b in a]\n",
    "    numerical_articles.append(list1)\n",
    "for h in tokenized_headings:\n",
    "  list1 = [dictionary_word_code[b] for b in h]\n",
    "  numerical_headings.append(list1)\n",
    "  \n",
    "padded_numerical_articles = []\n",
    "padded_numerical_headings = []\n",
    "for a in numerical_articles:\n",
    "  zeros = [0]*(max_length_article - len(a))\n",
    "  list1 = a + zeros\n",
    "  padded_numerical_articles.append(list1)\n",
    "for h in numerical_headings:\n",
    "  zeros = [0]*(max_length_heading - len(h))\n",
    "  list1 = h + zeros\n",
    "  padded_numerical_headings.append(list1)\n",
    "  \n",
    "with open(\"input_data.pickle\", \"wb\") as f:\n",
    "    pickle.dump((embedding_matrix,padded_numerical_articles,padded_numerical_headings,vocab_list), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0eeO97yHjAp"
   },
   "source": [
    "# Too much RAM was used so start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nfbmW_XAPL8"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(\"input_data.pickle\",\"rb\") as f:\n",
    "  _ , articles , headings, vocab_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13xfVE8i54mR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def next_batch(articles,headings,batch_size,num_time_steps,num_output_steps):                 #850     #84\n",
    "  \n",
    "  snipped_articles = []\n",
    "  snipped_headings = []\n",
    "  \n",
    "  for a in articles:\n",
    "    part_article = a[0:num_time_steps]\n",
    "    snipped_articles.append(part_article)\n",
    "    \n",
    "  random = np.random.randint(0,len(articles) - batch_size -1)\n",
    "  batch_x = np.array(snipped_articles[random:random+batch_size]).reshape([batch_size,num_time_steps,1])\n",
    "  onehotlistarticles = []\n",
    "  for article in batch_x:\n",
    "    for num in article:\n",
    "      zeros = np.array([0]*len(vocab_list))\n",
    "      zeros[num] = 1\n",
    "      onehotlistarticles.append(zeros)\n",
    "  onehotlistarticles = np.array(onehotlistarticles).reshape([batch_size,num_time_steps,-1])\n",
    "  \n",
    "  for h in headings:\n",
    "    part_heading = h[0:num_output_steps]\n",
    "    snipped_headings.append(part_heading)\n",
    "  \n",
    "  batch_y = np.array(snipped_headings[random:random+batch_size]).reshape([batch_size,num_output_steps,1])\n",
    "  onehotlistheadings = []\n",
    "  for heading in batch_y:\n",
    "    for num in heading:\n",
    "      zeros = np.array([0]*len(vocab_list))\n",
    "      zeros[num] = 1\n",
    "      onehotlistheadings.append(zeros)\n",
    "  onehotlistheadings = np.array(onehotlistheadings).reshape([batch_size,num_output_steps,-1])\n",
    "  \n",
    "  return onehotlistarticles , onehotlistheadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsM5HHVZp1AK"
   },
   "outputs": [],
   "source": [
    "onehotencodedarticle , onehotencodedheading = next_batch(articles,headings,1,50,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1553279355165,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "brEdaUprqVIY",
    "outputId": "7b415c55-00c7-44fd-d1f5-e5951021c685"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 165701)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotencodedarticle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1553279324869,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "84xf5k1nqUpK",
    "outputId": "293f074a-03bd-486e-d711-3bfba51a2426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotencodedheading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qhn7_KS4xxsK"
   },
   "source": [
    "# TensorFlow Graph \n",
    "\n",
    "## Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMtm5eFZmJ4_"
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "  used = tf.reduce_max(sequence, 2)\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHspC9d0uAdB"
   },
   "source": [
    "**See Imext Notebook for explaination of tf.reduce_max and how axis collapses and stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7w2duU_-psS5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvHe14VYgfi-"
   },
   "outputs": [],
   "source": [
    "num_time_steps = 100             #850\n",
    "num_output_steps = 10            #84\n",
    "dimensions_rnn_cell = 100\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSy6Z9ul_hCk"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUlstKfuiH_l"
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,shape = [None,num_time_steps,len(vocab_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lg6xJZRGATVd"
   },
   "source": [
    "** Right now too tired, not initializing with Embedding Matrix as its a pain , have no clue what the fuck the function wants from me, come back later here if the performance is not so good , see the documentation for initializer in LSTMCell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7262,
     "status": "ok",
     "timestamp": 1553949466353,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "NpXY4lXuX8JI",
    "outputId": "8274c3e7-b6db-4399-ef53-fe3a38c2c557"
   },
   "outputs": [],
   "source": [
    "lstm_fw_cell = [tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell),tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)] #,tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)]\n",
    "lstm_bw_cell = [tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell),tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)] #,tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell)]\n",
    "\n",
    "outputs, output_state_fw, output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, X, dtype='float32', sequence_length = length(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXwjpyP4DcLb"
   },
   "source": [
    "** Cool so session crashes after using all the RAM**\n",
    "\n",
    "** For testing if this is working or not **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBs4xL9Sy6Vs"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  batch_x, v = next_batch(articles,headings,batch_size,100,10)\n",
    "  combined_bi_rnn_output = sess.run(outputs,feed_dict = {X:batch_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1146,
     "status": "ok",
     "timestamp": 1553759327090,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "njEL3Nz0y8TD",
    "outputId": "c2021343-0ed2-441f-a5ec-a811bfab1bdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.2783952e-04, -6.8792311e-04, -9.4794697e-04, ...,\n",
       "          7.4256625e-04,  9.5902121e-04, -2.3523576e-03],\n",
       "        [ 7.2198868e-04, -1.6717522e-03, -9.4186328e-04, ...,\n",
       "         -9.8459411e-04,  2.8810717e-04, -1.9490661e-03],\n",
       "        [-2.4890318e-04, -2.2269606e-03, -8.7335058e-05, ...,\n",
       "         -2.5667732e-03, -2.6391957e-05, -8.0228376e-04],\n",
       "        ...,\n",
       "        [ 1.0020640e-03,  1.0294392e-03, -8.2223839e-04, ...,\n",
       "         -2.1115739e-03,  1.8902525e-03,  1.9151869e-04],\n",
       "        [ 4.7159812e-04,  6.7479024e-04, -8.5636129e-04, ...,\n",
       "         -1.6434949e-03,  7.3492981e-04,  1.9513143e-03],\n",
       "        [-4.7449372e-04,  9.3540055e-04,  3.5114659e-04, ...,\n",
       "         -1.4197577e-03, -2.0201574e-04,  1.1504232e-03]],\n",
       "\n",
       "       [[ 1.2039823e-03, -4.0472256e-05, -1.8650545e-04, ...,\n",
       "          2.3192090e-03,  8.3211134e-04,  9.6437446e-04],\n",
       "        [ 9.0374786e-04, -1.1380591e-03, -8.6801325e-04, ...,\n",
       "          1.5123325e-03,  1.6813138e-03,  4.9060979e-04],\n",
       "        [ 1.8986216e-03, -1.5466526e-03, -1.6539117e-03, ...,\n",
       "          2.1285641e-03,  1.5774365e-03,  1.3030614e-03],\n",
       "        ...,\n",
       "        [-2.1774797e-03,  9.8165800e-04,  5.9714838e-04, ...,\n",
       "         -2.6007753e-03, -1.9765771e-03,  8.1775035e-04],\n",
       "        [-1.8061906e-03, -6.3917111e-04,  1.5408400e-03, ...,\n",
       "         -1.9479378e-03, -2.0020495e-03,  3.4823653e-04],\n",
       "        [-9.8474650e-04,  2.8832370e-04, -2.2452020e-04, ...,\n",
       "         -9.5691578e-04, -9.4062556e-04,  1.7971134e-04]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_bi_rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEsTD4P27-QQ"
   },
   "outputs": [],
   "source": [
    "## Attention Mechanism : For now, wont be taking input from the final output of the network and lets see how it performs, afterwards take as input from the output of the final stage too \n",
    "\n",
    "## I think thats possible because the final outputs are also placeholders right? So that can be input to your 'neural_network' function\n",
    "\n",
    "### Define a Neural Network basically 200 : 100 : 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTQlXl13vWAU"
   },
   "outputs": [],
   "source": [
    "combined_BiRNN_outputs = tf.placeholder(tf.float32,shape = [None,num_time_steps,2*dimensions_rnn_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANT71WCevWAX"
   },
   "outputs": [],
   "source": [
    "num_hidden_neurons = 100\n",
    "def neural_network(combined_BiRnn_output):\n",
    "    \n",
    "    Wx1 = tf.Variable(tf.random_normal([batch_size,2*dimensions_rnn_cell,num_hidden_neurons],0.3,1/200))\n",
    "    Wx2 = tf.Variable(tf.random_normal([batch_size,num_hidden_neurons,num_output_steps],0.5,1/50))\n",
    "    b1 = tf.Variable(tf.constant(0.5,shape = [num_hidden_neurons]))\n",
    "    b2 = tf.Variable(tf.constant(0.3,shape = [num_output_steps]))\n",
    "    first_out = tf.nn.tanh(tf.add(tf.matmul(combined_BiRnn_output,Wx1),b1))\n",
    "    e = tf.nn.relu(tf.add(tf.matmul(first_out,Wx2),b2))\n",
    "        \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VQDdOravWAZ"
   },
   "outputs": [],
   "source": [
    "def softmax(e_values):\n",
    "    j = tf.exp(e_values)\n",
    "    totals = tf.reduce_sum(j,1)\n",
    "    j = tf.unstack(j,axis = 0)\n",
    "    totals = tf.unstack(totals, axis = 0)\n",
    "    for i in range(batch_size):\n",
    "        j[i] = j[i]/totals[i]\n",
    "    j = tf.stack(j,axis = 0)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DlE0DQWqvWAc"
   },
   "outputs": [],
   "source": [
    "e_vals = neural_network(combined_BiRNN_outputs)\n",
    "alpha_values = softmax(e_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkN_bf17vWAi"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVD3ollgvWAk"
   },
   "source": [
    "** To test whether alpha values are coming or not **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeu9WinxvWAm"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_x, _ = next_batch(articles,headings,batch_size,num_time_steps,num_output_steps)\n",
    "    outs = sess.run(outputs,feed_dict = {X: batch_x})\n",
    "    \n",
    "    alpha = sess.run(alpha_values,feed_dict = {combined_BiRNN_outputs:outs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1124,
     "status": "ok",
     "timestamp": 1553759448339,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "QUmgO-0WR59c",
    "outputId": "9c1022d3-8596-4075-852f-273dde46e6cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01067784, 0.01068803, 0.01068037, ..., 0.01068256,\n",
       "         0.01068129, 0.01068229],\n",
       "        [0.01016384, 0.01016348, 0.01016377, ..., 0.01016385,\n",
       "         0.01016377, 0.01016391],\n",
       "        [0.0093227 , 0.00930638, 0.00931866, ..., 0.00931506,\n",
       "         0.00931708, 0.0093158 ],\n",
       "        ...,\n",
       "        [0.01075403, 0.01076575, 0.01075705, ..., 0.01075957,\n",
       "         0.0107582 , 0.01075913],\n",
       "        [0.01092826, 0.01094368, 0.01093221, ..., 0.01093544,\n",
       "         0.01093367, 0.01093498],\n",
       "        [0.01121162, 0.0112332 , 0.01121708, ..., 0.01122183,\n",
       "         0.01121918, 0.01122093]],\n",
       "\n",
       "       [[0.00924081, 0.00923804, 0.00923832, ..., 0.00923288,\n",
       "         0.00924248, 0.00923921],\n",
       "        [0.00843764, 0.0084326 , 0.00843306, ..., 0.00842303,\n",
       "         0.00844053, 0.00843464],\n",
       "        [0.0093433 , 0.00934097, 0.00934119, ..., 0.00933637,\n",
       "         0.00934468, 0.00934188],\n",
       "        ...,\n",
       "        [0.00954852, 0.00954695, 0.00954719, ..., 0.00954347,\n",
       "         0.00954944, 0.00954765],\n",
       "        [0.01097711, 0.01098008, 0.0109798 , ..., 0.01098587,\n",
       "         0.01097534, 0.01097888],\n",
       "        [0.01110885, 0.01111211, 0.01111185, ..., 0.01111893,\n",
       "         0.01110685, 0.01111078]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0d2ZC8X_vWAp"
   },
   "source": [
    "** Weighted Average **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H73NLW_vWAr"
   },
   "outputs": [],
   "source": [
    "def weighted_average(outputs,alphas):\n",
    "    \n",
    "    alphas = tf.transpose(alphas,perm = [0,2,1])\n",
    "    context_vectors = tf.matmul(alphas,outputs)\n",
    "    \n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuuK1tQtvWAv"
   },
   "outputs": [],
   "source": [
    "context_vectors = weighted_average(combined_BiRNN_outputs,alpha_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ7b1tolvWA1"
   },
   "source": [
    "### Another idea you could do is instead of one hot encoding the input words use the embedding matrix you created, though wont be as useful if you could load up the embedding matrix as the weight vector it may be better than normal one hot encoding\n",
    "\n",
    "### Or use a LSTM cell as the first layer and top it off with 2 BiRnn layers or something if LSTM cell supports initialzing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vhu5DFy1vWA3"
   },
   "outputs": [],
   "source": [
    "context = tf.placeholder(tf.float32,shape = [batch_size,num_output_steps,2*dimensions_rnn_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGKxTECavWBD"
   },
   "outputs": [],
   "source": [
    "cell_factory = tf.nn.rnn_cell.LSTMCell(dimensions_rnn_cell,activation=tf.nn.relu)\n",
    "initial_state = cell_factory.zero_state(batch_size, dtype=tf.float32)\n",
    "decoder, _ = tf.nn.dynamic_rnn(cell_factory,context,initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKDYU-NLvWBJ"
   },
   "outputs": [],
   "source": [
    "def output_neural_network(decoder_rnn):\n",
    "    \n",
    "    Wx1 = tf.Variable(tf.random_normal([batch_size,dimensions_rnn_cell,len(vocab_list)],0.5,1/dimensions_rnn_cell))\n",
    "    b1 = tf.Variable(tf.constant(0.5,shape = [len(vocab_list)]))\n",
    "    first_out = tf.nn.softmax(tf.nn.relu((tf.add(tf.matmul(decoder_rnn,Wx1),b1))))\n",
    "        \n",
    "    return first_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSpDOiUbvWBO"
   },
   "outputs": [],
   "source": [
    "probability_distribution = output_neural_network(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_az7h5NSxPet"
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32,shape = [None,num_output_steps,len(vocab_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqQ8dtuOvWBT"
   },
   "outputs": [],
   "source": [
    "c_e = tf.nn.softmax_cross_entropy_with_logits_v2(logits = probability_distribution,labels = y_true)\n",
    "mse = tf.reduce_mean(tf.square(probability_distribution-y_true))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 5)\n",
    "train = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdhC7Ih5nDo6"
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "v7KyPVBWvWBX",
    "outputId": "16c3166f-543e-4f06-dbf8-408c99e5ef54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.03493e-06\n",
      "6.0349294e-06\n",
      "6.0349284e-06\n",
      "6.034928e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-7592c19251a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mbatch_x\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheadings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_time_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_output_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcombined_BiRNN_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  batch_x , batch_y = next_batch(articles,headings,batch_size,num_time_steps,num_output_steps)\n",
    "  for i in range(100):\n",
    "    outs = sess.run(outputs, feed_dict = {X:batch_x})\n",
    "    con = sess.run(context_vectors, feed_dict = {combined_BiRNN_outputs:outs})\n",
    "    sess.run(train, feed_dict = {context:con,y_true:batch_y})\n",
    "    \n",
    "    if i%10 ==0:\n",
    "      print(sess.run(mse,feed_dict = {context:con,y_true:batch_y}))\n",
    "      \n",
    "  saver.save(sess,'./imext_summarizer.ckpt')\n",
    "      \n",
    "    #batch_x , batch_y = next_batch(articles,headings,1,100,10)\n",
    "    #outs = sess.run(outputs, feed_dict = {X:batch_x})\n",
    "    #con = sess.run(context_vectors, feed_dict = {combined_BiRNN_outputs:outs})\n",
    "    #final_output = sess.run(probability_distribution, feed_dict = {context:con})\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1378,
     "status": "ok",
     "timestamp": 1553793564468,
     "user": {
      "displayName": "Jeet Thaker",
      "photoUrl": "https://lh4.googleusercontent.com/-jNjv1rf_vkQ/AAAAAAAAAAI/AAAAAAAAACk/hecSejvkvPM/s64/photo.jpg",
      "userId": "18423117888903233743"
     },
     "user_tz": -330
    },
    "id": "wDaBB_NLyrLl",
    "outputId": "695708ed-28c7-49a1-f7e0-edeef9abe8bb"
   },
   "outputs": [],
   "source": [
    "# final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1kIjphCT9vC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Summarization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
